{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of IFT6760-H20-A1-Template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreeadeac22/IFT6760H20/blob/master/Copy_of_IFT6760_H20_A1_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM9fD394B_Pe",
        "colab_type": "text"
      },
      "source": [
        "This synthetic MDP has no particular meaning except that it produces pretty plot. It is inspired from [Dadashi & al. (2019)](http://proceedings.mlr.press/v97/dadashi19a.html).\n",
        "\n",
        "Note: we will be using [JAX](https://jax.readthedocs.io/en/latest/) instead of Numpy because we will later make use of the automatic differentiation feature. \n",
        "Because JAX is meant to work best when the code is properly [*jitted*](https://en.wikipedia.org/wiki/Just-in-time_compilation) (which I didn't do for simplicity), some sections may run slower than a pure Numpy implementation. It shouldn't be a problem for the problems that we are working with (order of few seconds max). If you are impatient, feel free to use the [@jit](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit) decorator where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqvBWr_ROOgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "def synthetic_mdp():\n",
        "  P = np.array([[[0.75 , 0.25 ], [0.2 , 0.8 ]],\n",
        "                [[0.99, 0.01], [0.8, 0.2]]])\n",
        "  R = np.array(([[-0.5, -0.25 ],\n",
        "                [ 0.5 ,  0.25 ]]))\n",
        "  return P, R, 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Ra2WWSBj20",
        "colab_type": "text"
      },
      "source": [
        "The method of successive approximation provides us with a generic template for all of our algorithms. In the following, we will use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRSuNAuEPOQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_iterates(xinit, operator, termination_condition):\n",
        "    x, xprev = operator(xinit), xinit\n",
        "    yield x\n",
        "    while not termination_condition(xprev, x):\n",
        "        x, xprev = operator(x), x\n",
        "        yield x\n",
        "\n",
        "def successive_approximation(xinit, operator=lambda x: x, termination_condition=lambda xprev, x: False):\n",
        "    for iterate in generate_iterates(xinit, operator, termination_condition):\n",
        "      pass\n",
        "    return iterate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzlHLc5YKKR3",
        "colab_type": "text"
      },
      "source": [
        "We define a generic termination condition for successive approximation based on the Euclidean distance between two iterates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXkVO95t6ERS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def default_termination(xprev, x, epsilon=1e-8):\n",
        "  return np.linalg.norm(xprev - x) < epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUkjR0Ak9H0X",
        "colab_type": "text"
      },
      "source": [
        "# Policy Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nblOb8_RJQOp",
        "colab_type": "text"
      },
      "source": [
        "We start by implementing the policy evaluation operator. Refer to the section on vector notation in the course notes or read Puterman (1994) for references. \n",
        "In general, many of these operations can be written consicely by using [np.einsum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html). For an overview of Einstein notation, see this [blogpost](https://rockt.github.io/2018/04/30/einsum) by Tim RocktÃ¤schel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsF5eokZOhxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_policy_evaluation_operator(P, R, discount, policy):\n",
        "  def policy_evaluation_operator(v):  \n",
        "    p_pi = np.einsum('ijk,ji->jk', P, policy)\n",
        "    pv = np.einsum('ij,j->i', p_pi, v)\n",
        "    discounted_pv = np.multiply(discount, pv)\n",
        "    r_pi = np.einsum('ij, ij -> i', policy, R)\n",
        "    lv = np.add(r_pi, discounted_pv)\n",
        "    print(\"lv \", lv)\n",
        "    return lv\n",
        "  return policy_evaluation_operator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A61DGS9zPPW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdp = synthetic_mdp()\n",
        "P, _, discount = mdp\n",
        "nstates, nactions = P.shape[-1], P.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iam11W2I7Jor",
        "colab_type": "text"
      },
      "source": [
        "A good sanity check to see if our policy evaluation method is to use a reward function which is $1$ everywhere. In the discounted setting, we have that the expected return should be equal to $1/(1- \n",
        "\\gamma)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgRmfX6x5Qyo",
        "colab_type": "code",
        "outputId": "0dcfadbf-c0f2-4439-97d1-bb096708bb5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "reward_all_ones = np.ones((nstates, nactions))\n",
        "uniform_policy = np.ones((nstates, nactions))/nactions\n",
        "\n",
        "policy_evaluation_operator = make_policy_evaluation_operator(P, reward_all_ones, discount, uniform_policy)\n",
        "solution = successive_approximation(np.zeros((nstates,)), policy_evaluation_operator, default_termination)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lv  [1. 1.]\n",
            "lv  [1.9 1.9]\n",
            "lv  [2.71 2.71]\n",
            "lv  [3.439 3.439]\n",
            "lv  [4.0951 4.0951]\n",
            "lv  [4.68559 4.68559]\n",
            "lv  [5.217031 5.217031]\n",
            "lv  [5.6953279 5.6953279]\n",
            "lv  [6.12579511 6.12579511]\n",
            "lv  [6.5132156 6.5132156]\n",
            "lv  [6.86189404 6.86189404]\n",
            "lv  [7.17570464 7.17570464]\n",
            "lv  [7.45813417 7.45813417]\n",
            "lv  [7.71232075 7.71232075]\n",
            "lv  [7.94108868 7.94108868]\n",
            "lv  [8.14697981 8.14697981]\n",
            "lv  [8.33228183 8.33228183]\n",
            "lv  [8.49905365 8.49905365]\n",
            "lv  [8.64914828 8.64914828]\n",
            "lv  [8.78423345 8.78423345]\n",
            "lv  [8.90581011 8.90581011]\n",
            "lv  [9.0152291 9.0152291]\n",
            "lv  [9.11370619 9.11370619]\n",
            "lv  [9.20233557 9.20233557]\n",
            "lv  [9.28210201 9.28210201]\n",
            "lv  [9.35389181 9.35389181]\n",
            "lv  [9.41850263 9.41850263]\n",
            "lv  [9.47665237 9.47665237]\n",
            "lv  [9.52898713 9.52898713]\n",
            "lv  [9.57608842 9.57608842]\n",
            "lv  [9.61847958 9.61847958]\n",
            "lv  [9.65663162 9.65663162]\n",
            "lv  [9.69096846 9.69096846]\n",
            "lv  [9.72187161 9.72187161]\n",
            "lv  [9.74968445 9.74968445]\n",
            "lv  [9.774716 9.774716]\n",
            "lv  [9.7972444 9.7972444]\n",
            "lv  [9.81751996 9.81751996]\n",
            "lv  [9.83576797 9.83576797]\n",
            "lv  [9.85219117 9.85219117]\n",
            "lv  [9.86697205 9.86697205]\n",
            "lv  [9.88027485 9.88027485]\n",
            "lv  [9.89224736 9.89224736]\n",
            "lv  [9.90302263 9.90302263]\n",
            "lv  [9.91272036 9.91272036]\n",
            "lv  [9.92144833 9.92144833]\n",
            "lv  [9.9293035 9.9293035]\n",
            "lv  [9.93637315 9.93637315]\n",
            "lv  [9.94273583 9.94273583]\n",
            "lv  [9.94846225 9.94846225]\n",
            "lv  [9.95361602 9.95361602]\n",
            "lv  [9.95825442 9.95825442]\n",
            "lv  [9.96242898 9.96242898]\n",
            "lv  [9.96618608 9.96618608]\n",
            "lv  [9.96956747 9.96956747]\n",
            "lv  [9.97261073 9.97261073]\n",
            "lv  [9.97534965 9.97534965]\n",
            "lv  [9.97781469 9.97781469]\n",
            "lv  [9.98003322 9.98003322]\n",
            "lv  [9.9820299 9.9820299]\n",
            "lv  [9.98382691 9.98382691]\n",
            "lv  [9.98544422 9.98544422]\n",
            "lv  [9.98689979 9.98689979]\n",
            "lv  [9.98820982 9.98820982]\n",
            "lv  [9.98938883 9.98938883]\n",
            "lv  [9.99044995 9.99044995]\n",
            "lv  [9.99140496 9.99140496]\n",
            "lv  [9.99226446 9.99226446]\n",
            "lv  [9.99303801 9.99303801]\n",
            "lv  [9.99373421 9.99373421]\n",
            "lv  [9.99436079 9.99436079]\n",
            "lv  [9.99492471 9.99492471]\n",
            "lv  [9.99543224 9.99543224]\n",
            "lv  [9.99588902 9.99588902]\n",
            "lv  [9.99630012 9.99630012]\n",
            "lv  [9.9966701 9.9966701]\n",
            "lv  [9.99700309 9.99700309]\n",
            "lv  [9.99730278 9.99730278]\n",
            "lv  [9.99757251 9.99757251]\n",
            "lv  [9.99781525 9.99781525]\n",
            "lv  [9.99803373 9.99803373]\n",
            "lv  [9.99823036 9.99823036]\n",
            "lv  [9.99840732 9.99840732]\n",
            "lv  [9.99856659 9.99856659]\n",
            "lv  [9.99870993 9.99870993]\n",
            "lv  [9.99883894 9.99883894]\n",
            "lv  [9.99895504 9.99895504]\n",
            "lv  [9.99905954 9.99905954]\n",
            "lv  [9.99915359 9.99915359]\n",
            "lv  [9.99923823 9.99923823]\n",
            "lv  [9.9993144 9.9993144]\n",
            "lv  [9.99938296 9.99938296]\n",
            "lv  [9.99944467 9.99944467]\n",
            "lv  [9.9995002 9.9995002]\n",
            "lv  [9.99955018 9.99955018]\n",
            "lv  [9.99959516 9.99959516]\n",
            "lv  [9.99963565 9.99963565]\n",
            "lv  [9.99967208 9.99967208]\n",
            "lv  [9.99970487 9.99970487]\n",
            "lv  [9.99973439 9.99973439]\n",
            "lv  [9.99976095 9.99976095]\n",
            "lv  [9.99978485 9.99978485]\n",
            "lv  [9.99980637 9.99980637]\n",
            "lv  [9.99982573 9.99982573]\n",
            "lv  [9.99984316 9.99984316]\n",
            "lv  [9.99985884 9.99985884]\n",
            "lv  [9.99987296 9.99987296]\n",
            "lv  [9.99988566 9.99988566]\n",
            "lv  [9.9998971 9.9998971]\n",
            "lv  [9.99990739 9.99990739]\n",
            "lv  [9.99991665 9.99991665]\n",
            "lv  [9.99992498 9.99992498]\n",
            "lv  [9.99993248 9.99993248]\n",
            "lv  [9.99993924 9.99993924]\n",
            "lv  [9.99994531 9.99994531]\n",
            "lv  [9.99995078 9.99995078]\n",
            "lv  [9.9999557 9.9999557]\n",
            "lv  [9.99996013 9.99996013]\n",
            "lv  [9.99996412 9.99996412]\n",
            "lv  [9.99996771 9.99996771]\n",
            "lv  [9.99997094 9.99997094]\n",
            "lv  [9.99997384 9.99997384]\n",
            "lv  [9.99997646 9.99997646]\n",
            "lv  [9.99997881 9.99997881]\n",
            "lv  [9.99998093 9.99998093]\n",
            "lv  [9.99998284 9.99998284]\n",
            "lv  [9.99998455 9.99998455]\n",
            "lv  [9.9999861 9.9999861]\n",
            "lv  [9.99998749 9.99998749]\n",
            "lv  [9.99998874 9.99998874]\n",
            "lv  [9.99998987 9.99998987]\n",
            "lv  [9.99999088 9.99999088]\n",
            "lv  [9.99999179 9.99999179]\n",
            "lv  [9.99999261 9.99999261]\n",
            "lv  [9.99999335 9.99999335]\n",
            "lv  [9.99999402 9.99999402]\n",
            "lv  [9.99999461 9.99999461]\n",
            "lv  [9.99999515 9.99999515]\n",
            "lv  [9.99999564 9.99999564]\n",
            "lv  [9.99999607 9.99999607]\n",
            "lv  [9.99999647 9.99999647]\n",
            "lv  [9.99999682 9.99999682]\n",
            "lv  [9.99999714 9.99999714]\n",
            "lv  [9.99999742 9.99999742]\n",
            "lv  [9.99999768 9.99999768]\n",
            "lv  [9.99999791 9.99999791]\n",
            "lv  [9.99999812 9.99999812]\n",
            "lv  [9.99999831 9.99999831]\n",
            "lv  [9.99999848 9.99999848]\n",
            "lv  [9.99999863 9.99999863]\n",
            "lv  [9.99999877 9.99999877]\n",
            "lv  [9.99999889 9.99999889]\n",
            "lv  [9.999999 9.999999]\n",
            "lv  [9.9999991 9.9999991]\n",
            "lv  [9.99999919 9.99999919]\n",
            "lv  [9.99999927 9.99999927]\n",
            "lv  [9.99999935 9.99999935]\n",
            "lv  [9.99999941 9.99999941]\n",
            "lv  [9.99999947 9.99999947]\n",
            "lv  [9.99999952 9.99999952]\n",
            "lv  [9.99999957 9.99999957]\n",
            "lv  [9.99999961 9.99999961]\n",
            "lv  [9.99999965 9.99999965]\n",
            "lv  [9.99999969 9.99999969]\n",
            "lv  [9.99999972 9.99999972]\n",
            "lv  [9.99999975 9.99999975]\n",
            "lv  [9.99999977 9.99999977]\n",
            "lv  [9.99999979 9.99999979]\n",
            "lv  [9.99999982 9.99999982]\n",
            "lv  [9.99999983 9.99999983]\n",
            "lv  [9.99999985 9.99999985]\n",
            "lv  [9.99999987 9.99999987]\n",
            "lv  [9.99999988 9.99999988]\n",
            "lv  [9.99999989 9.99999989]\n",
            "lv  [9.9999999 9.9999999]\n",
            "lv  [9.99999991 9.99999991]\n",
            "lv  [9.99999992 9.99999992]\n",
            "lv  [9.99999993 9.99999993]\n",
            "lv  [9.99999994 9.99999994]\n",
            "lv  [9.99999994 9.99999994]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfs7k0_n731a",
        "colab_type": "text"
      },
      "source": [
        "The following should return true:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nHxm0XA7fae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7b11ab1c-b13b-4af6-ecbc-9d2a669138f8"
      },
      "source": [
        "exp_result = [1/(1-discount)]*nstates\n",
        "print(\"exp_result \", exp_result)\n",
        "np.allclose(solution, [1/(1-discount)]*nstates)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "exp_result  [10.000000000000002, 10.000000000000002]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(True, dtype=bool)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzBiQoj48Div",
        "colab_type": "text"
      },
      "source": [
        "Now we can also make sure that the solution found by succesive approximation matches the closed-form solution which we would obtain using a direct method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asgiFX-z8C8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def direct_policy_evaluation(P, R, discount, policy):\n",
        "  # Add your implementation below. Avoid explicit matrix inverses\n",
        "  # You should be able to do this in 3 lines or less.\n",
        "  p_pi = np.einsum('ijk,ji->jk', P, policy)\n",
        "  discounted_p_pi = np.multiply(discount, p_pi)\n",
        "  a = np.subtract(np.identity(P.shape[-1]), discounted_p_pi)\n",
        "\n",
        "  r_pi = np.einsum('ij, ij -> i', policy, R)\n",
        "  return np.linalg.solve(a, r_pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKBPMSD58qex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "solution = direct_policy_evaluation(P, reward_all_ones, discount, uniform_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebRl21Rk88EU",
        "colab_type": "text"
      },
      "source": [
        "This should once again return true: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yySrLUSO8-BH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4ec4510-9a0b-4e68-9062-9b8650f66fe7"
      },
      "source": [
        "np.allclose(solution, [1/(1-discount)]*nstates)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(True, dtype=bool)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWdYO4Pk7SA2",
        "colab_type": "text"
      },
      "source": [
        "## Projected Policy Evaluation Operator\n",
        "\n",
        "We have seen that the projected policy evaluation equations can be solved either in closed form or iteratively using the projected (Bellman) policy evaluation operator. \n",
        "\n",
        "Because the projection is taken with respect to a weighted Euclidean distance, you have to compute the entries of the diagonal matrix $X$ by solving for the stationary distribution of the given policy in the MDP. \n",
        "\n",
        "Remember that a stationary distribution $x$ must satisfy $x^\\top P = x^\\top$, \n",
        "where $x^\\top \\mathbf{1} = 1 \\in \\mathbb{R}^{|\\mathcal{S}|}$ and $P \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{S}|}$. Therfore, $x^\\top$ is a left eigenvector of $P$. By the [Perron-Frobenius theorem](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem), we know that there exists a eigenvalue of maximum modulus which is a real positive number: the Perron root. For stochastic matrices, the Perron root (the spectral radis) is equal to 1. Furthermore, the eigenvector associated with the Perron root (the Perron vector) has only positive components. \n",
        "\n",
        "You can use numpy to find the eigendecomposition of the stochastic matrix, isolate the Perron vector (through the Perron root) and normalize the resulting vector so that it sums up to one. \n",
        "\n",
        "Alternatively, you could also choose to view the problem of finding a solution to $x^\\top P = x^\\top$ as a fixed-point problem and use the above ``successive_approximation`` function with the appropriate operator. Note however that the zero vector is a trivial solution of this problem, and you should make sure not to initialize from this point.\n",
        "\n",
        "In the following, implement a function to compute the stationary distribution for a given transition matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPQsu8DMtfdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stationary_distribution(P):\n",
        "  # Compute stationary distribution here\n",
        "  w, v = np.linalg.eig(P.T)\n",
        "  for i, e in enumerate(w):\n",
        "    if np.isclose(e, 1.):\n",
        "      return v[:,i] / np.sum(v[:,i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp4olEZDtiiX",
        "colab_type": "text"
      },
      "source": [
        "We can now use this function to pre-compute the stationary distribution and use it to form the projected policy evaluation operator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s9_7Pc4PtB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_pvi_operator(P, R, discount, policy, phi):\n",
        "  # Compute A and b matrices here\n",
        "  p_pi = np.einsum('ijk,ji->jk', P, policy)\n",
        "\n",
        "  x = stationary_distribution(p_pi)\n",
        "  diagx = np.diag(x)\n",
        " \n",
        "  phitx = np.einsum('ij,jk->ik', phi.T, diagx)\n",
        "\n",
        "  discounted_p_pi = np.multiply(discount, p_pi)\n",
        "  i = np.identity(P.shape[-1])\n",
        "  a = np.subtract(i, discounted_p_pi)\n",
        "  phitx_a = np.einsum('ij, jk->ik', phitx, a)\n",
        "  A = np.einsum('ij, jk->ik', phitx_a, phi)\n",
        "  #print(\"A \", A)\n",
        "\n",
        "  r_pi = np.einsum('ij, ij -> i', policy, R)\n",
        "  b = np.einsum('ij, j->i', phitx, r_pi)\n",
        "  #print(\"b \", b)\n",
        "\n",
        "  def pvi_operator(w):\n",
        "    # Compute projection here, without forming matrix inverse explicitely\n",
        "    phitx_phi = np.einsum('ij, jk->ik', phitx, phi)\n",
        "    #print(\"phitx_phi \", phitx_phi)\n",
        "    #print(\"w \", w)\n",
        "    c_wk = np.einsum('ij, j->i', phitx_phi, w)\n",
        "    #print(\"c_wk \", c_wk)\n",
        "\n",
        "    A_wk = np.einsum('ij, j->i', A, w)\n",
        "\n",
        "    rhs = np.subtract( np.add(c_wk, b), A_wk)\n",
        "    new_w = np.linalg.solve(phitx_phi, rhs)\n",
        "    print(\"new_w \", new_w)\n",
        "    return new_w\n",
        "  return pvi_operator  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhkXt45ROshY",
        "colab_type": "text"
      },
      "source": [
        "As a sanity check, we can verify that we obtain the exact solution when using *one-hot*  (tabular) features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpDJX2HE7g8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_features = np.eye(nstates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyV5tIUS7cN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a910bcb-e3b2-46f1-9641-2924ce038bec"
      },
      "source": [
        "pvi_operator = make_pvi_operator(P, reward_all_ones, discount, uniform_policy, one_hot_features)\n",
        "solution = successive_approximation(np.zeros(nstates), pvi_operator, default_termination)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new_w  [1.+0.j 1.+0.j]\n",
            "new_w  [1.9+0.j 1.9+0.j]\n",
            "new_w  [2.71+0.j 2.71+0.j]\n",
            "new_w  [3.439+0.j 3.439+0.j]\n",
            "new_w  [4.0951+0.j 4.0951+0.j]\n",
            "new_w  [4.68559+0.j 4.68559+0.j]\n",
            "new_w  [5.217031+0.j 5.217031+0.j]\n",
            "new_w  [5.6953279+0.j 5.6953279+0.j]\n",
            "new_w  [6.12579511+0.j 6.12579511+0.j]\n",
            "new_w  [6.5132156+0.j 6.5132156+0.j]\n",
            "new_w  [6.86189404+0.j 6.86189404+0.j]\n",
            "new_w  [7.17570464+0.j 7.17570464+0.j]\n",
            "new_w  [7.45813417+0.j 7.45813417+0.j]\n",
            "new_w  [7.71232075+0.j 7.71232075+0.j]\n",
            "new_w  [7.94108868+0.j 7.94108868+0.j]\n",
            "new_w  [8.14697981+0.j 8.14697981+0.j]\n",
            "new_w  [8.33228183+0.j 8.33228183+0.j]\n",
            "new_w  [8.49905365+0.j 8.49905365+0.j]\n",
            "new_w  [8.64914828+0.j 8.64914828+0.j]\n",
            "new_w  [8.78423345+0.j 8.78423345+0.j]\n",
            "new_w  [8.90581011+0.j 8.90581011+0.j]\n",
            "new_w  [9.0152291+0.j 9.0152291+0.j]\n",
            "new_w  [9.11370619+0.j 9.11370619+0.j]\n",
            "new_w  [9.20233557+0.j 9.20233557+0.j]\n",
            "new_w  [9.28210201+0.j 9.28210201+0.j]\n",
            "new_w  [9.35389181+0.j 9.35389181+0.j]\n",
            "new_w  [9.41850263+0.j 9.41850263+0.j]\n",
            "new_w  [9.47665237+0.j 9.47665237+0.j]\n",
            "new_w  [9.52898713+0.j 9.52898713+0.j]\n",
            "new_w  [9.57608842+0.j 9.57608842+0.j]\n",
            "new_w  [9.61847958+0.j 9.61847958+0.j]\n",
            "new_w  [9.65663162+0.j 9.65663162+0.j]\n",
            "new_w  [9.69096846+0.j 9.69096846+0.j]\n",
            "new_w  [9.72187161+0.j 9.72187161+0.j]\n",
            "new_w  [9.74968445+0.j 9.74968445+0.j]\n",
            "new_w  [9.774716+0.j 9.774716+0.j]\n",
            "new_w  [9.7972444+0.j 9.7972444+0.j]\n",
            "new_w  [9.81751996+0.j 9.81751996+0.j]\n",
            "new_w  [9.83576797+0.j 9.83576797+0.j]\n",
            "new_w  [9.85219117+0.j 9.85219117+0.j]\n",
            "new_w  [9.86697205+0.j 9.86697205+0.j]\n",
            "new_w  [9.88027485+0.j 9.88027485+0.j]\n",
            "new_w  [9.89224736+0.j 9.89224736+0.j]\n",
            "new_w  [9.90302263+0.j 9.90302263+0.j]\n",
            "new_w  [9.91272036+0.j 9.91272036+0.j]\n",
            "new_w  [9.92144833+0.j 9.92144833+0.j]\n",
            "new_w  [9.9293035+0.j 9.9293035+0.j]\n",
            "new_w  [9.93637315+0.j 9.93637315+0.j]\n",
            "new_w  [9.94273583+0.j 9.94273583+0.j]\n",
            "new_w  [9.94846225+0.j 9.94846225+0.j]\n",
            "new_w  [9.95361602+0.j 9.95361602+0.j]\n",
            "new_w  [9.95825442+0.j 9.95825442+0.j]\n",
            "new_w  [9.96242898+0.j 9.96242898+0.j]\n",
            "new_w  [9.96618608+0.j 9.96618608+0.j]\n",
            "new_w  [9.96956747+0.j 9.96956747+0.j]\n",
            "new_w  [9.97261073+0.j 9.97261073+0.j]\n",
            "new_w  [9.97534965+0.j 9.97534965+0.j]\n",
            "new_w  [9.97781469+0.j 9.97781469+0.j]\n",
            "new_w  [9.98003322+0.j 9.98003322+0.j]\n",
            "new_w  [9.9820299+0.j 9.9820299+0.j]\n",
            "new_w  [9.98382691+0.j 9.98382691+0.j]\n",
            "new_w  [9.98544422+0.j 9.98544422+0.j]\n",
            "new_w  [9.98689979+0.j 9.98689979+0.j]\n",
            "new_w  [9.98820982+0.j 9.98820982+0.j]\n",
            "new_w  [9.98938883+0.j 9.98938883+0.j]\n",
            "new_w  [9.99044995+0.j 9.99044995+0.j]\n",
            "new_w  [9.99140496+0.j 9.99140496+0.j]\n",
            "new_w  [9.99226446+0.j 9.99226446+0.j]\n",
            "new_w  [9.99303801+0.j 9.99303801+0.j]\n",
            "new_w  [9.99373421+0.j 9.99373421+0.j]\n",
            "new_w  [9.99436079+0.j 9.99436079+0.j]\n",
            "new_w  [9.99492471+0.j 9.99492471+0.j]\n",
            "new_w  [9.99543224+0.j 9.99543224+0.j]\n",
            "new_w  [9.99588902+0.j 9.99588902+0.j]\n",
            "new_w  [9.99630012+0.j 9.99630012+0.j]\n",
            "new_w  [9.9966701+0.j 9.9966701+0.j]\n",
            "new_w  [9.99700309+0.j 9.99700309+0.j]\n",
            "new_w  [9.99730278+0.j 9.99730278+0.j]\n",
            "new_w  [9.99757251+0.j 9.99757251+0.j]\n",
            "new_w  [9.99781525+0.j 9.99781525+0.j]\n",
            "new_w  [9.99803373+0.j 9.99803373+0.j]\n",
            "new_w  [9.99823036+0.j 9.99823036+0.j]\n",
            "new_w  [9.99840732+0.j 9.99840732+0.j]\n",
            "new_w  [9.99856659+0.j 9.99856659+0.j]\n",
            "new_w  [9.99870993+0.j 9.99870993+0.j]\n",
            "new_w  [9.99883894+0.j 9.99883894+0.j]\n",
            "new_w  [9.99895504+0.j 9.99895504+0.j]\n",
            "new_w  [9.99905954+0.j 9.99905954+0.j]\n",
            "new_w  [9.99915359+0.j 9.99915359+0.j]\n",
            "new_w  [9.99923823+0.j 9.99923823+0.j]\n",
            "new_w  [9.9993144+0.j 9.9993144+0.j]\n",
            "new_w  [9.99938296+0.j 9.99938296+0.j]\n",
            "new_w  [9.99944467+0.j 9.99944467+0.j]\n",
            "new_w  [9.9995002+0.j 9.9995002+0.j]\n",
            "new_w  [9.99955018+0.j 9.99955018+0.j]\n",
            "new_w  [9.99959516+0.j 9.99959516+0.j]\n",
            "new_w  [9.99963565+0.j 9.99963565+0.j]\n",
            "new_w  [9.99967208+0.j 9.99967208+0.j]\n",
            "new_w  [9.99970487+0.j 9.99970487+0.j]\n",
            "new_w  [9.99973439+0.j 9.99973439+0.j]\n",
            "new_w  [9.99976095+0.j 9.99976095+0.j]\n",
            "new_w  [9.99978485+0.j 9.99978485+0.j]\n",
            "new_w  [9.99980637+0.j 9.99980637+0.j]\n",
            "new_w  [9.99982573+0.j 9.99982573+0.j]\n",
            "new_w  [9.99984316+0.j 9.99984316+0.j]\n",
            "new_w  [9.99985884+0.j 9.99985884+0.j]\n",
            "new_w  [9.99987296+0.j 9.99987296+0.j]\n",
            "new_w  [9.99988566+0.j 9.99988566+0.j]\n",
            "new_w  [9.9998971+0.j 9.9998971+0.j]\n",
            "new_w  [9.99990739+0.j 9.99990739+0.j]\n",
            "new_w  [9.99991665+0.j 9.99991665+0.j]\n",
            "new_w  [9.99992498+0.j 9.99992498+0.j]\n",
            "new_w  [9.99993248+0.j 9.99993248+0.j]\n",
            "new_w  [9.99993924+0.j 9.99993924+0.j]\n",
            "new_w  [9.99994531+0.j 9.99994531+0.j]\n",
            "new_w  [9.99995078+0.j 9.99995078+0.j]\n",
            "new_w  [9.9999557+0.j 9.9999557+0.j]\n",
            "new_w  [9.99996013+0.j 9.99996013+0.j]\n",
            "new_w  [9.99996412+0.j 9.99996412+0.j]\n",
            "new_w  [9.99996771+0.j 9.99996771+0.j]\n",
            "new_w  [9.99997094+0.j 9.99997094+0.j]\n",
            "new_w  [9.99997384+0.j 9.99997384+0.j]\n",
            "new_w  [9.99997646+0.j 9.99997646+0.j]\n",
            "new_w  [9.99997881+0.j 9.99997881+0.j]\n",
            "new_w  [9.99998093+0.j 9.99998093+0.j]\n",
            "new_w  [9.99998284+0.j 9.99998284+0.j]\n",
            "new_w  [9.99998455+0.j 9.99998455+0.j]\n",
            "new_w  [9.9999861+0.j 9.9999861+0.j]\n",
            "new_w  [9.99998749+0.j 9.99998749+0.j]\n",
            "new_w  [9.99998874+0.j 9.99998874+0.j]\n",
            "new_w  [9.99998987+0.j 9.99998987+0.j]\n",
            "new_w  [9.99999088+0.j 9.99999088+0.j]\n",
            "new_w  [9.99999179+0.j 9.99999179+0.j]\n",
            "new_w  [9.99999261+0.j 9.99999261+0.j]\n",
            "new_w  [9.99999335+0.j 9.99999335+0.j]\n",
            "new_w  [9.99999402+0.j 9.99999402+0.j]\n",
            "new_w  [9.99999461+0.j 9.99999461+0.j]\n",
            "new_w  [9.99999515+0.j 9.99999515+0.j]\n",
            "new_w  [9.99999564+0.j 9.99999564+0.j]\n",
            "new_w  [9.99999607+0.j 9.99999607+0.j]\n",
            "new_w  [9.99999647+0.j 9.99999647+0.j]\n",
            "new_w  [9.99999682+0.j 9.99999682+0.j]\n",
            "new_w  [9.99999714+0.j 9.99999714+0.j]\n",
            "new_w  [9.99999742+0.j 9.99999742+0.j]\n",
            "new_w  [9.99999768+0.j 9.99999768+0.j]\n",
            "new_w  [9.99999791+0.j 9.99999791+0.j]\n",
            "new_w  [9.99999812+0.j 9.99999812+0.j]\n",
            "new_w  [9.99999831+0.j 9.99999831+0.j]\n",
            "new_w  [9.99999848+0.j 9.99999848+0.j]\n",
            "new_w  [9.99999863+0.j 9.99999863+0.j]\n",
            "new_w  [9.99999877+0.j 9.99999877+0.j]\n",
            "new_w  [9.99999889+0.j 9.99999889+0.j]\n",
            "new_w  [9.999999+0.j 9.999999+0.j]\n",
            "new_w  [9.9999991+0.j 9.9999991+0.j]\n",
            "new_w  [9.99999919+0.j 9.99999919+0.j]\n",
            "new_w  [9.99999927+0.j 9.99999927+0.j]\n",
            "new_w  [9.99999935+0.j 9.99999935+0.j]\n",
            "new_w  [9.99999941+0.j 9.99999941+0.j]\n",
            "new_w  [9.99999947+0.j 9.99999947+0.j]\n",
            "new_w  [9.99999952+0.j 9.99999952+0.j]\n",
            "new_w  [9.99999957+0.j 9.99999957+0.j]\n",
            "new_w  [9.99999961+0.j 9.99999961+0.j]\n",
            "new_w  [9.99999965+0.j 9.99999965+0.j]\n",
            "new_w  [9.99999969+0.j 9.99999969+0.j]\n",
            "new_w  [9.99999972+0.j 9.99999972+0.j]\n",
            "new_w  [9.99999975+0.j 9.99999975+0.j]\n",
            "new_w  [9.99999977+0.j 9.99999977+0.j]\n",
            "new_w  [9.99999979+0.j 9.99999979+0.j]\n",
            "new_w  [9.99999982+0.j 9.99999982+0.j]\n",
            "new_w  [9.99999983+0.j 9.99999983+0.j]\n",
            "new_w  [9.99999985+0.j 9.99999985+0.j]\n",
            "new_w  [9.99999987+0.j 9.99999987+0.j]\n",
            "new_w  [9.99999988+0.j 9.99999988+0.j]\n",
            "new_w  [9.99999989+0.j 9.99999989+0.j]\n",
            "new_w  [9.9999999+0.j 9.9999999+0.j]\n",
            "new_w  [9.99999991+0.j 9.99999991+0.j]\n",
            "new_w  [9.99999992+0.j 9.99999992+0.j]\n",
            "new_w  [9.99999993+0.j 9.99999993+0.j]\n",
            "new_w  [9.99999994+0.j 9.99999994+0.j]\n",
            "new_w  [9.99999994+0.j 9.99999994+0.j]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO-O2CcN7rPP",
        "colab_type": "text"
      },
      "source": [
        "Because we used the reward function which is one everywhere, we should also find that the expected return is $1/(1-\\gamma)$. \n",
        "\n",
        "This test should also return true:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDr6icJh7ulj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "26b33e98-5ceb-4485-c197-5288f05ecde8"
      },
      "source": [
        "print(\"one_hot_features @ solution \", one_hot_features @ solution)\n",
        "print(\"[1/(1-discount)]*nstates \", [1/(1-discount)]*nstates)\n",
        "np.allclose(one_hot_features @ solution, [1/(1-discount)]*nstates)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one_hot_features @ solution  [9.99999994+0.j 9.99999994+0.j]\n",
            "[1/(1-discount)]*nstates  [10.000000000000002, 10.000000000000002]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(True, dtype=bool)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXzvaE3sv8c3",
        "colab_type": "text"
      },
      "source": [
        "A non-iterative alternative to the above PVI algorithm is simply to solve for the projected policy evaluation equations using a direct method. Implement the projected counterpart to the above ``direct_policy_evaluation`` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV52XImau62l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def direct_projected_policy_evaluation(P, R, discount, policy, phi):\n",
        "  # Compute A and b matrices here\n",
        "  p_pi = np.einsum('ijk,ji->jk', P, policy)\n",
        "\n",
        "  x = stationary_distribution(p_pi)\n",
        "  diagx = np.diag(x)\n",
        " \n",
        "  phitx = np.einsum('ij,jk->ik', phi.T, diagx)\n",
        "\n",
        "  discounted_p_pi = np.multiply(discount, p_pi)\n",
        "  i = np.identity(P.shape[-1])\n",
        "  a = np.subtract(i, discounted_p_pi)\n",
        "  phitx_a = np.einsum('ij, jk->ik', phitx, a)\n",
        "  A = np.einsum('ij, jk->ik', phitx_a, phi)\n",
        "\n",
        "  r_pi = np.einsum('ij, ij -> i', policy, R)\n",
        "  b = np.einsum('ij, j->i', phitx, r_pi)\n",
        "  return np.linalg.solve(A, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJWu8bOxvrlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "solution = direct_projected_policy_evaluation(P, reward_all_ones, discount, uniform_policy, one_hot_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ocbZkcv6pV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b93ed62c-92fe-48fe-e0f2-74bb0c9499de"
      },
      "source": [
        "np.allclose(one_hot_features @ solution, [1/(1-discount)]*nstates)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(True, dtype=bool)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "absD_AHc6Hr2",
        "colab_type": "text"
      },
      "source": [
        "### Bertsekas Bound\n",
        "\n",
        "Verify that the bound in proposition 6.3.1 for Bertsekas holds. Use the direct of iterative policy evaluation methods to compute all the terms in the bound. \n",
        "\n",
        "Test for the synthetic MDP with a uniform policy and tabular features. Then generate a random full-rank matrix of features and repeat the exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKSjhO1f4ci-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# verifying Bertsekas bound\n",
        "\n",
        "\n",
        "# solution = direct_policy_evaluation(P, reward_all_ones, discount, uniform_policy)\n",
        "# projected_solution = direct_projected_policy_evaluation(P, reward_all_ones, discount, uniform_policy, one_hot_features)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyMzMoLO9OWt",
        "colab_type": "text"
      },
      "source": [
        "# Optimality Equations\n",
        "\n",
        "As usual, I advise you to use [np.einsum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) to implement the following optimality operators. Note that in all cases, the following operators act on vectors $v \\in \\mathbb{R}^{|\\mathcal{S}|}$ and not on Q-factors $Q \\in \\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{A}|}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wobqDMBU9Yd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_bellman_optimality_operator(P, R, discount):\n",
        "  def bellman_optimality_operator(v):\n",
        "    # This should be a one-liner\n",
        "    pv = np.einsum('ijk,k->ij', P, v)\n",
        "    discounted_pv = np.multiply(discount, pv)\n",
        "    lv = np.add(R, discounted_pv)\n",
        "    max_a = np.max(lv, axis=0)\n",
        "    print(\"max_a \", max_a)\n",
        "    return max_a\n",
        "  return bellman_optimality_operator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7VKL_yRRmNn",
        "colab_type": "text"
      },
      "source": [
        "Now do the same thing but where you use the [soft-max](https://en.wikipedia.org/wiki/Smooth_maximum) (smooth maximum) instead of the *hard* one. You can use the built-in implementations of the soft-max function for more stability. See [Rust (1996)](https://doi.org/10.1016/s1574-0021(96)01016-7) for reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiM5UyHqPSRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_smooth_bellman_optimality_operator(P, R, discount, temperature):\n",
        "  def smooth_bellman_optimality_operator(v):\n",
        "    # This should be a one-liner\n",
        "    pv = np.einsum('ijk,k->ij', P, v)\n",
        "    discounted_pv = np.multiply(discount, pv)\n",
        "    temp_lv = 1./temperature * np.add(R, discounted_pv)\n",
        "\n",
        "    max_el = np.max(temp_lv)\n",
        "    sumOfExp = np.sum(np.exp(temp_lv - max_el), axis=0)\n",
        "    log_sumexp = max_el + np.log(sumOfExp)\n",
        "    \n",
        "    soft_max_a = temperature * log_sumexp\n",
        "    #print(\"soft_max_a \", soft_max_a)\n",
        "    return soft_max_a\n",
        "  return smooth_bellman_optimality_operator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4jtWev0BHKR",
        "colab_type": "text"
      },
      "source": [
        "We can apply the same sanity check for the optimality equations and use a reward function where all components are set to $1$. The following loop goes over the *hard* and smooth optimality operators and verifies that we recover the optimal value function in all cases:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpeftUNq-Q8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95d6b49d-2840-4133-8470-186a2471c2c7"
      },
      "source": [
        "default_temperature = 1e-5\n",
        "\n",
        "hard_operator = make_bellman_optimality_operator(P, reward_all_ones, discount)\n",
        "smooth_operator = make_smooth_bellman_optimality_operator(P, reward_all_ones, discount, default_temperature)\n",
        "\n",
        "for operator in [hard_operator, smooth_operator]:\n",
        "  solution = successive_approximation(np.zeros((nstates,)), operator, default_termination)\n",
        "  print(np.allclose(solution, [1/(1-discount)]*nstates))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_a  [1. 1.]\n",
            "max_a  [1.9 1.9]\n",
            "max_a  [2.71 2.71]\n",
            "max_a  [3.439 3.439]\n",
            "max_a  [4.0951 4.0951]\n",
            "max_a  [4.68559 4.68559]\n",
            "max_a  [5.217031 5.217031]\n",
            "max_a  [5.6953279 5.6953279]\n",
            "max_a  [6.12579511 6.12579511]\n",
            "max_a  [6.5132156 6.5132156]\n",
            "max_a  [6.86189404 6.86189404]\n",
            "max_a  [7.17570464 7.17570464]\n",
            "max_a  [7.45813417 7.45813417]\n",
            "max_a  [7.71232075 7.71232075]\n",
            "max_a  [7.94108868 7.94108868]\n",
            "max_a  [8.14697981 8.14697981]\n",
            "max_a  [8.33228183 8.33228183]\n",
            "max_a  [8.49905365 8.49905365]\n",
            "max_a  [8.64914828 8.64914828]\n",
            "max_a  [8.78423345 8.78423345]\n",
            "max_a  [8.90581011 8.90581011]\n",
            "max_a  [9.0152291 9.0152291]\n",
            "max_a  [9.11370619 9.11370619]\n",
            "max_a  [9.20233557 9.20233557]\n",
            "max_a  [9.28210201 9.28210201]\n",
            "max_a  [9.35389181 9.35389181]\n",
            "max_a  [9.41850263 9.41850263]\n",
            "max_a  [9.47665237 9.47665237]\n",
            "max_a  [9.52898713 9.52898713]\n",
            "max_a  [9.57608842 9.57608842]\n",
            "max_a  [9.61847958 9.61847958]\n",
            "max_a  [9.65663162 9.65663162]\n",
            "max_a  [9.69096846 9.69096846]\n",
            "max_a  [9.72187161 9.72187161]\n",
            "max_a  [9.74968445 9.74968445]\n",
            "max_a  [9.774716 9.774716]\n",
            "max_a  [9.7972444 9.7972444]\n",
            "max_a  [9.81751996 9.81751996]\n",
            "max_a  [9.83576797 9.83576797]\n",
            "max_a  [9.85219117 9.85219117]\n",
            "max_a  [9.86697205 9.86697205]\n",
            "max_a  [9.88027485 9.88027485]\n",
            "max_a  [9.89224736 9.89224736]\n",
            "max_a  [9.90302263 9.90302263]\n",
            "max_a  [9.91272036 9.91272036]\n",
            "max_a  [9.92144833 9.92144833]\n",
            "max_a  [9.9293035 9.9293035]\n",
            "max_a  [9.93637315 9.93637315]\n",
            "max_a  [9.94273583 9.94273583]\n",
            "max_a  [9.94846225 9.94846225]\n",
            "max_a  [9.95361602 9.95361602]\n",
            "max_a  [9.95825442 9.95825442]\n",
            "max_a  [9.96242898 9.96242898]\n",
            "max_a  [9.96618608 9.96618608]\n",
            "max_a  [9.96956747 9.96956747]\n",
            "max_a  [9.97261073 9.97261073]\n",
            "max_a  [9.97534965 9.97534965]\n",
            "max_a  [9.97781469 9.97781469]\n",
            "max_a  [9.98003322 9.98003322]\n",
            "max_a  [9.9820299 9.9820299]\n",
            "max_a  [9.98382691 9.98382691]\n",
            "max_a  [9.98544422 9.98544422]\n",
            "max_a  [9.98689979 9.98689979]\n",
            "max_a  [9.98820982 9.98820982]\n",
            "max_a  [9.98938883 9.98938883]\n",
            "max_a  [9.99044995 9.99044995]\n",
            "max_a  [9.99140496 9.99140496]\n",
            "max_a  [9.99226446 9.99226446]\n",
            "max_a  [9.99303801 9.99303801]\n",
            "max_a  [9.99373421 9.99373421]\n",
            "max_a  [9.99436079 9.99436079]\n",
            "max_a  [9.99492471 9.99492471]\n",
            "max_a  [9.99543224 9.99543224]\n",
            "max_a  [9.99588902 9.99588902]\n",
            "max_a  [9.99630012 9.99630012]\n",
            "max_a  [9.9966701 9.9966701]\n",
            "max_a  [9.99700309 9.99700309]\n",
            "max_a  [9.99730278 9.99730278]\n",
            "max_a  [9.99757251 9.99757251]\n",
            "max_a  [9.99781525 9.99781525]\n",
            "max_a  [9.99803373 9.99803373]\n",
            "max_a  [9.99823036 9.99823036]\n",
            "max_a  [9.99840732 9.99840732]\n",
            "max_a  [9.99856659 9.99856659]\n",
            "max_a  [9.99870993 9.99870993]\n",
            "max_a  [9.99883894 9.99883894]\n",
            "max_a  [9.99895504 9.99895504]\n",
            "max_a  [9.99905954 9.99905954]\n",
            "max_a  [9.99915359 9.99915359]\n",
            "max_a  [9.99923823 9.99923823]\n",
            "max_a  [9.9993144 9.9993144]\n",
            "max_a  [9.99938296 9.99938296]\n",
            "max_a  [9.99944467 9.99944467]\n",
            "max_a  [9.9995002 9.9995002]\n",
            "max_a  [9.99955018 9.99955018]\n",
            "max_a  [9.99959516 9.99959516]\n",
            "max_a  [9.99963565 9.99963565]\n",
            "max_a  [9.99967208 9.99967208]\n",
            "max_a  [9.99970487 9.99970487]\n",
            "max_a  [9.99973439 9.99973439]\n",
            "max_a  [9.99976095 9.99976095]\n",
            "max_a  [9.99978485 9.99978485]\n",
            "max_a  [9.99980637 9.99980637]\n",
            "max_a  [9.99982573 9.99982573]\n",
            "max_a  [9.99984316 9.99984316]\n",
            "max_a  [9.99985884 9.99985884]\n",
            "max_a  [9.99987296 9.99987296]\n",
            "max_a  [9.99988566 9.99988566]\n",
            "max_a  [9.9998971 9.9998971]\n",
            "max_a  [9.99990739 9.99990739]\n",
            "max_a  [9.99991665 9.99991665]\n",
            "max_a  [9.99992498 9.99992498]\n",
            "max_a  [9.99993248 9.99993248]\n",
            "max_a  [9.99993924 9.99993924]\n",
            "max_a  [9.99994531 9.99994531]\n",
            "max_a  [9.99995078 9.99995078]\n",
            "max_a  [9.9999557 9.9999557]\n",
            "max_a  [9.99996013 9.99996013]\n",
            "max_a  [9.99996412 9.99996412]\n",
            "max_a  [9.99996771 9.99996771]\n",
            "max_a  [9.99997094 9.99997094]\n",
            "max_a  [9.99997384 9.99997384]\n",
            "max_a  [9.99997646 9.99997646]\n",
            "max_a  [9.99997881 9.99997881]\n",
            "max_a  [9.99998093 9.99998093]\n",
            "max_a  [9.99998284 9.99998284]\n",
            "max_a  [9.99998455 9.99998455]\n",
            "max_a  [9.9999861 9.9999861]\n",
            "max_a  [9.99998749 9.99998749]\n",
            "max_a  [9.99998874 9.99998874]\n",
            "max_a  [9.99998987 9.99998987]\n",
            "max_a  [9.99999088 9.99999088]\n",
            "max_a  [9.99999179 9.99999179]\n",
            "max_a  [9.99999261 9.99999261]\n",
            "max_a  [9.99999335 9.99999335]\n",
            "max_a  [9.99999402 9.99999402]\n",
            "max_a  [9.99999461 9.99999461]\n",
            "max_a  [9.99999515 9.99999515]\n",
            "max_a  [9.99999564 9.99999564]\n",
            "max_a  [9.99999607 9.99999607]\n",
            "max_a  [9.99999647 9.99999647]\n",
            "max_a  [9.99999682 9.99999682]\n",
            "max_a  [9.99999714 9.99999714]\n",
            "max_a  [9.99999742 9.99999742]\n",
            "max_a  [9.99999768 9.99999768]\n",
            "max_a  [9.99999791 9.99999791]\n",
            "max_a  [9.99999812 9.99999812]\n",
            "max_a  [9.99999831 9.99999831]\n",
            "max_a  [9.99999848 9.99999848]\n",
            "max_a  [9.99999863 9.99999863]\n",
            "max_a  [9.99999877 9.99999877]\n",
            "max_a  [9.99999889 9.99999889]\n",
            "max_a  [9.999999 9.999999]\n",
            "max_a  [9.9999991 9.9999991]\n",
            "max_a  [9.99999919 9.99999919]\n",
            "max_a  [9.99999927 9.99999927]\n",
            "max_a  [9.99999935 9.99999935]\n",
            "max_a  [9.99999941 9.99999941]\n",
            "max_a  [9.99999947 9.99999947]\n",
            "max_a  [9.99999952 9.99999952]\n",
            "max_a  [9.99999957 9.99999957]\n",
            "max_a  [9.99999961 9.99999961]\n",
            "max_a  [9.99999965 9.99999965]\n",
            "max_a  [9.99999969 9.99999969]\n",
            "max_a  [9.99999972 9.99999972]\n",
            "max_a  [9.99999975 9.99999975]\n",
            "max_a  [9.99999977 9.99999977]\n",
            "max_a  [9.99999979 9.99999979]\n",
            "max_a  [9.99999982 9.99999982]\n",
            "max_a  [9.99999983 9.99999983]\n",
            "max_a  [9.99999985 9.99999985]\n",
            "max_a  [9.99999987 9.99999987]\n",
            "max_a  [9.99999988 9.99999988]\n",
            "max_a  [9.99999989 9.99999989]\n",
            "max_a  [9.9999999 9.9999999]\n",
            "max_a  [9.99999991 9.99999991]\n",
            "max_a  [9.99999992 9.99999992]\n",
            "max_a  [9.99999993 9.99999993]\n",
            "max_a  [9.99999994 9.99999994]\n",
            "max_a  [9.99999994 9.99999994]\n",
            "True\n",
            "soft_max_a  [1.00000693 1.00000693]\n",
            "soft_max_a  [1.90001317 1.90001317]\n",
            "soft_max_a  [2.71001878 2.71001878]\n",
            "soft_max_a  [3.43902384 3.43902384]\n",
            "soft_max_a  [4.09512839 4.09512839]\n",
            "soft_max_a  [4.68562248 4.68562248]\n",
            "soft_max_a  [5.21706716 5.21706716]\n",
            "soft_max_a  [5.69536738 5.69536738]\n",
            "soft_max_a  [6.12583757 6.12583757]\n",
            "soft_max_a  [6.51326075 6.51326075]\n",
            "soft_max_a  [6.8619416 6.8619416]\n",
            "soft_max_a  [7.17575437 7.17575437]\n",
            "soft_max_a  [7.45818587 7.45818587]\n",
            "soft_max_a  [7.71237421 7.71237421]\n",
            "soft_max_a  [7.94114372 7.94114372]\n",
            "soft_max_a  [8.14703628 8.14703628]\n",
            "soft_max_a  [8.33233959 8.33233959]\n",
            "soft_max_a  [8.49911256 8.49911256]\n",
            "soft_max_a  [8.64920823 8.64920823]\n",
            "soft_max_a  [8.78429434 8.78429434]\n",
            "soft_max_a  [8.90587184 8.90587184]\n",
            "soft_max_a  [9.01529159 9.01529159]\n",
            "soft_max_a  [9.11376936 9.11376936]\n",
            "soft_max_a  [9.20239935 9.20239935]\n",
            "soft_max_a  [9.28216635 9.28216635]\n",
            "soft_max_a  [9.35395665 9.35395665]\n",
            "soft_max_a  [9.41856791 9.41856791]\n",
            "soft_max_a  [9.47671805 9.47671805]\n",
            "soft_max_a  [9.52905318 9.52905318]\n",
            "soft_max_a  [9.57615479 9.57615479]\n",
            "soft_max_a  [9.61854625 9.61854625]\n",
            "soft_max_a  [9.65669855 9.65669855]\n",
            "soft_max_a  [9.69103563 9.69103563]\n",
            "soft_max_a  [9.721939 9.721939]\n",
            "soft_max_a  [9.74975203 9.74975203]\n",
            "soft_max_a  [9.77478376 9.77478376]\n",
            "soft_max_a  [9.79731231 9.79731231]\n",
            "soft_max_a  [9.81758801 9.81758801]\n",
            "soft_max_a  [9.83583614 9.83583614]\n",
            "soft_max_a  [9.85225946 9.85225946]\n",
            "soft_max_a  [9.86704045 9.86704045]\n",
            "soft_max_a  [9.88034333 9.88034333]\n",
            "soft_max_a  [9.89231593 9.89231593]\n",
            "soft_max_a  [9.90309127 9.90309127]\n",
            "soft_max_a  [9.91278907 9.91278907]\n",
            "soft_max_a  [9.9215171 9.9215171]\n",
            "soft_max_a  [9.92937232 9.92937232]\n",
            "soft_max_a  [9.93644202 9.93644202]\n",
            "soft_max_a  [9.94280475 9.94280475]\n",
            "soft_max_a  [9.94853121 9.94853121]\n",
            "soft_max_a  [9.95368502 9.95368502]\n",
            "soft_max_a  [9.95832345 9.95832345]\n",
            "soft_max_a  [9.96249803 9.96249803]\n",
            "soft_max_a  [9.96625516 9.96625516]\n",
            "soft_max_a  [9.96963658 9.96963658]\n",
            "soft_max_a  [9.97267985 9.97267985]\n",
            "soft_max_a  [9.9754188 9.9754188]\n",
            "soft_max_a  [9.97788385 9.97788385]\n",
            "soft_max_a  [9.9801024 9.9801024]\n",
            "soft_max_a  [9.98209909 9.98209909]\n",
            "soft_max_a  [9.98389611 9.98389611]\n",
            "soft_max_a  [9.98551343 9.98551343]\n",
            "soft_max_a  [9.98696902 9.98696902]\n",
            "soft_max_a  [9.98827905 9.98827905]\n",
            "soft_max_a  [9.98945808 9.98945808]\n",
            "soft_max_a  [9.9905192 9.9905192]\n",
            "soft_max_a  [9.99147421 9.99147421]\n",
            "soft_max_a  [9.99233372 9.99233372]\n",
            "soft_max_a  [9.99310728 9.99310728]\n",
            "soft_max_a  [9.99380348 9.99380348]\n",
            "soft_max_a  [9.99443007 9.99443007]\n",
            "soft_max_a  [9.99499399 9.99499399]\n",
            "soft_max_a  [9.99550152 9.99550152]\n",
            "soft_max_a  [9.9959583 9.9959583]\n",
            "soft_max_a  [9.9963694 9.9963694]\n",
            "soft_max_a  [9.9967394 9.9967394]\n",
            "soft_max_a  [9.99707239 9.99707239]\n",
            "soft_max_a  [9.99737208 9.99737208]\n",
            "soft_max_a  [9.9976418 9.9976418]\n",
            "soft_max_a  [9.99788455 9.99788455]\n",
            "soft_max_a  [9.99810303 9.99810303]\n",
            "soft_max_a  [9.99829966 9.99829966]\n",
            "soft_max_a  [9.99847662 9.99847662]\n",
            "soft_max_a  [9.99863589 9.99863589]\n",
            "soft_max_a  [9.99877924 9.99877924]\n",
            "soft_max_a  [9.99890824 9.99890824]\n",
            "soft_max_a  [9.99902435 9.99902435]\n",
            "soft_max_a  [9.99912885 9.99912885]\n",
            "soft_max_a  [9.99922289 9.99922289]\n",
            "soft_max_a  [9.99930754 9.99930754]\n",
            "soft_max_a  [9.99938371 9.99938371]\n",
            "soft_max_a  [9.99945227 9.99945227]\n",
            "soft_max_a  [9.99951398 9.99951398]\n",
            "soft_max_a  [9.99956951 9.99956951]\n",
            "soft_max_a  [9.99961949 9.99961949]\n",
            "soft_max_a  [9.99966447 9.99966447]\n",
            "soft_max_a  [9.99970496 9.99970496]\n",
            "soft_max_a  [9.99974139 9.99974139]\n",
            "soft_max_a  [9.99977419 9.99977419]\n",
            "soft_max_a  [9.9998037 9.9998037]\n",
            "soft_max_a  [9.99983026 9.99983026]\n",
            "soft_max_a  [9.99985417 9.99985417]\n",
            "soft_max_a  [9.99987568 9.99987568]\n",
            "soft_max_a  [9.99989504 9.99989504]\n",
            "soft_max_a  [9.99991247 9.99991247]\n",
            "soft_max_a  [9.99992816 9.99992816]\n",
            "soft_max_a  [9.99994227 9.99994227]\n",
            "soft_max_a  [9.99995498 9.99995498]\n",
            "soft_max_a  [9.99996641 9.99996641]\n",
            "soft_max_a  [9.9999767 9.9999767]\n",
            "soft_max_a  [9.99998596 9.99998596]\n",
            "soft_max_a  [9.9999943 9.9999943]\n",
            "soft_max_a  [10.0000018 10.0000018]\n",
            "soft_max_a  [10.00000855 10.00000855]\n",
            "soft_max_a  [10.00001463 10.00001463]\n",
            "soft_max_a  [10.0000201 10.0000201]\n",
            "soft_max_a  [10.00002502 10.00002502]\n",
            "soft_max_a  [10.00002945 10.00002945]\n",
            "soft_max_a  [10.00003343 10.00003343]\n",
            "soft_max_a  [10.00003702 10.00003702]\n",
            "soft_max_a  [10.00004025 10.00004025]\n",
            "soft_max_a  [10.00004316 10.00004316]\n",
            "soft_max_a  [10.00004577 10.00004577]\n",
            "soft_max_a  [10.00004813 10.00004813]\n",
            "soft_max_a  [10.00005025 10.00005025]\n",
            "soft_max_a  [10.00005215 10.00005215]\n",
            "soft_max_a  [10.00005387 10.00005387]\n",
            "soft_max_a  [10.00005541 10.00005541]\n",
            "soft_max_a  [10.0000568 10.0000568]\n",
            "soft_max_a  [10.00005805 10.00005805]\n",
            "soft_max_a  [10.00005918 10.00005918]\n",
            "soft_max_a  [10.00006019 10.00006019]\n",
            "soft_max_a  [10.00006111 10.00006111]\n",
            "soft_max_a  [10.00006193 10.00006193]\n",
            "soft_max_a  [10.00006267 10.00006267]\n",
            "soft_max_a  [10.00006333 10.00006333]\n",
            "soft_max_a  [10.00006393 10.00006393]\n",
            "soft_max_a  [10.00006447 10.00006447]\n",
            "soft_max_a  [10.00006495 10.00006495]\n",
            "soft_max_a  [10.00006539 10.00006539]\n",
            "soft_max_a  [10.00006578 10.00006578]\n",
            "soft_max_a  [10.00006613 10.00006613]\n",
            "soft_max_a  [10.00006645 10.00006645]\n",
            "soft_max_a  [10.00006674 10.00006674]\n",
            "soft_max_a  [10.000067 10.000067]\n",
            "soft_max_a  [10.00006723 10.00006723]\n",
            "soft_max_a  [10.00006744 10.00006744]\n",
            "soft_max_a  [10.00006762 10.00006762]\n",
            "soft_max_a  [10.00006779 10.00006779]\n",
            "soft_max_a  [10.00006795 10.00006795]\n",
            "soft_max_a  [10.00006808 10.00006808]\n",
            "soft_max_a  [10.00006821 10.00006821]\n",
            "soft_max_a  [10.00006832 10.00006832]\n",
            "soft_max_a  [10.00006842 10.00006842]\n",
            "soft_max_a  [10.00006851 10.00006851]\n",
            "soft_max_a  [10.00006859 10.00006859]\n",
            "soft_max_a  [10.00006866 10.00006866]\n",
            "soft_max_a  [10.00006873 10.00006873]\n",
            "soft_max_a  [10.00006878 10.00006878]\n",
            "soft_max_a  [10.00006884 10.00006884]\n",
            "soft_max_a  [10.00006889 10.00006889]\n",
            "soft_max_a  [10.00006893 10.00006893]\n",
            "soft_max_a  [10.00006897 10.00006897]\n",
            "soft_max_a  [10.000069 10.000069]\n",
            "soft_max_a  [10.00006903 10.00006903]\n",
            "soft_max_a  [10.00006906 10.00006906]\n",
            "soft_max_a  [10.00006909 10.00006909]\n",
            "soft_max_a  [10.00006911 10.00006911]\n",
            "soft_max_a  [10.00006913 10.00006913]\n",
            "soft_max_a  [10.00006915 10.00006915]\n",
            "soft_max_a  [10.00006916 10.00006916]\n",
            "soft_max_a  [10.00006918 10.00006918]\n",
            "soft_max_a  [10.00006919 10.00006919]\n",
            "soft_max_a  [10.00006921 10.00006921]\n",
            "soft_max_a  [10.00006922 10.00006922]\n",
            "soft_max_a  [10.00006923 10.00006923]\n",
            "soft_max_a  [10.00006924 10.00006924]\n",
            "soft_max_a  [10.00006924 10.00006924]\n",
            "soft_max_a  [10.00006925 10.00006925]\n",
            "soft_max_a  [10.00006926 10.00006926]\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSc1mIstWHGD",
        "colab_type": "text"
      },
      "source": [
        "Equipped with policy evaluation methods from above, we can also easily define a policy evaluation operator. Note that this time, the operator takes a policy (not a value vector) and returns an improved policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVM5q4rtWQtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_policy_iteration_operator(P, R, discount):\n",
        "  def policy_iteration_operator(policy):\n",
        "    v = direct_policy_evaluation(P, R, discount, policy)\n",
        "\n",
        "    pv = np.einsum('ijk,k->ij', P, v)\n",
        "    discounted_pv = np.multiply(discount, pv)\n",
        "    lv = np.add(R, discounted_pv)\n",
        "    argmax_a = np.argmax(lv, axis=0)\n",
        "    new_policy = np.eye(P.shape[-1])[argmax_a]\n",
        "\n",
        "    print(\"new pol \", new_policy)\n",
        "    return new_policy\n",
        "  return policy_iteration_operator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJbSf-pEkl4N",
        "colab_type": "text"
      },
      "source": [
        "We also have to define a new termination condition, suited to the fact that we iterate over deterministic policies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbvP63qokiPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_iteration_termination(policy_prev, policy):\n",
        "  return np.allclose(policy_prev, policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muzV3W9xkx1x",
        "colab_type": "text"
      },
      "source": [
        "In a somewhat redundant exercise, you can also define the smooth counterpart to the usual policy iteration by replacing the argmax with the soft-argmax. You can also use a built-in implementation for better numerical stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCzlaZsykjRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax.nn import softmax\n",
        "def make_smooth_policy_iteration_operator(P, R, discount, temperature):\n",
        "  def smooth_policy_iteration_operator(policy):\n",
        "    v = direct_policy_evaluation(P, R, discount, policy)\n",
        "\n",
        "    pv = np.einsum('ijk,k->ij', P, v)\n",
        "    discounted_pv = np.multiply(discount, pv)\n",
        "\n",
        "    temp_lv = 1./temperature * np.add(R, discounted_pv)\n",
        "\n",
        "\n",
        "    max_el = temp_lv.max()\n",
        "    normalized = np.exp(temp_lv - max_el)\n",
        "    \n",
        "    new_policy = temp_lv / np.sum(temp_lv, axis=0)\n",
        "\n",
        "    print(\"new pol \", new_policy)\n",
        "    return new_policy\n",
        "  return smooth_policy_iteration_operator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ViwtCKlBFq",
        "colab_type": "text"
      },
      "source": [
        "The method of sucessive approximation applied to these operators returns a policy as output, if we want to verify that they are indeed optimal, we now need to re-use our policy evaluation code to compute the associated value function.\n",
        "\n",
        "The following should return true twice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SrQ2ZEZZb73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "407143a2-e436-47b2-a719-14574809b3bc"
      },
      "source": [
        "policy_iteration_operator = make_policy_iteration_operator(P, reward_all_ones, discount)\n",
        "policy = successive_approximation(np.zeros((nstates, nactions)), policy_iteration_operator, policy_iteration_termination)\n",
        "solution = direct_policy_evaluation(P, reward_all_ones, discount, policy)\n",
        "print(np.allclose(solution, [1/(1-discount)]*nstates))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new pol  [[1. 0.]\n",
            " [1. 0.]]\n",
            "new pol  [[1. 0.]\n",
            " [1. 0.]]\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oqbEfFplicW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9177e9b0-9499-4de2-d022-36438efc8f63"
      },
      "source": [
        "smooth_policy_iteration_operator = make_smooth_policy_iteration_operator(P, reward_all_ones, discount, default_temperature)\n",
        "policy = successive_approximation(np.zeros((nstates, nactions)), smooth_policy_iteration_operator, default_termination)\n",
        "solution = direct_policy_evaluation(P, reward_all_ones, discount, policy)\n",
        "print(np.allclose(solution, [1/(1-discount)]*nstates))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new pol  [[0.5 0.5]\n",
            " [0.5 0.5]]\n",
            "new pol  [[0.5 0.5]\n",
            " [0.5 0.5]]\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06KXVif-wzlR",
        "colab_type": "text"
      },
      "source": [
        "## Newton-Kantorovich\n",
        "\n",
        "In order to implement Newton-Kantorovich for the smooth Bellman operator, we first use the analytical expression for its Gateaux derivative. See [Rust (1996)](https://doi.org/10.1016/s1574-0021(96)01016-7) for reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwaLGLLYOrZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax.experimental.stax import softmax\n",
        "\n",
        "def make_gateaux_derivative(P, R, discount, temperature):\n",
        "  \"\"\" Returns the Gateaux derivative at v of the smooth Bellman operator\n",
        "  \"\"\"\n",
        "  def gateaux_at(v):\n",
        "    \"\"\" Binds v into a function computing the gateaux derivative\n",
        "    \"\"\"\n",
        "    pv = np.einsum('ijk,k->ij', P, v)\n",
        "    discounted_pv = np.multiply(discount, pv)\n",
        "\n",
        "    temp_lv = 1./temperature * np.add(R, discounted_pv)\n",
        "    d = softmax(temp_lv)\n",
        "    #print(\"at v \", v)\n",
        "    def gateaux_direction(w):\n",
        "      \"\"\" Compute the Gateaux derivative at v in the direction of w\n",
        "      \"\"\"\n",
        "      # Add your code here\n",
        "      pw = np.einsum('ijk,k->ij', P, w)\n",
        "      d_pw = np.einsum('ij, ji -> i', d, pw)\n",
        "      discounted_dpw = discount * d_pw \n",
        "      return discounted_dpw\n",
        "    return gateaux_direction\n",
        "  return gateaux_at"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJOeFmVzPOLo",
        "colab_type": "text"
      },
      "source": [
        "We can also compute this derivative using forward-mode automatic differentiation in [JAX](https://github.com/google/jax). Under the hood, forward-mode automatic differentiation is implemented by defining the jacobian-vector for all primitive operations. This vector-jacobian product corresponds to our notion of directional derivative and the function [vjp](https://jax.readthedocs.io/en/latest/jax.html#jax.jvp) gives us that.\n",
        "\n",
        "The following should return true:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eno-wKxSDa6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a8230517-8a07-49cc-fea7-1fea7dc68305"
      },
      "source": [
        "from jax import jvp\n",
        "\n",
        "v = np.ones(nstates)/nstates\n",
        "w = np.array([1.,2.])\n",
        "\n",
        "_, tangents = jvp(smooth_operator, (v, ), (w,))\n",
        "deriv = make_gateaux_derivative(P, reward_all_ones, discount, default_temperature)(v)(w)\n",
        "\n",
        "print(\"tangents \", tangents)\n",
        "print(\"deriv \", deriv)\n",
        "np.allclose(tangents, deriv)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "soft_max_a  Traced<ConcreteArray([1.45000693 1.45000693])>with<JVPTrace(level=0/0)>\n",
            "tangents  [1.017 1.35 ]\n",
            "deriv  [1.017 1.35 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(True, dtype=bool)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwx2GuDETxbX",
        "colab_type": "text"
      },
      "source": [
        "Equipped with the Gateaux derivative, we can then implement the Newton-Kantorovich algorithm in a generic way. Because Newton-Kantorovich involves taking an inverse, we need to be careful as to how we implement this step. A naive approach would consists in forming the full Jacobian and directly calling [np.linalg.solve](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html). But we can do better! Using a classical iterative method, all we require is to be able to take matrix-vector products, or more abstractly, to be able to apply an operator to any vector.  \n",
        "\n",
        "In the following, you have to build such method by defining the appropriate operator. Note that this operator has the same form as the policy evalution one, but for $I - A$ and where $A$ need not be a stochastic matrix (but we still need the spectral radius $\\sigma(I - A) < 1$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG7PuddtWxjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_iterative_solver(A, b):\n",
        "  \"\"\" Solves $Ax = b$, assuming that the spectral radius of $I - A$ is \n",
        "  strictly less than 1. We assume that $A$ is a callable $A(v)$.\n",
        "  \"\"\"\n",
        "  def operator(x):\n",
        "    \"\"\" Basic iterative step without preconditioning \n",
        "    (you can add preconditioning if you feel like)\n",
        "\n",
        "    This is what you would obtain by writting the Neumann series epxansion of \n",
        "    $A$ recursively. \n",
        "    \"\"\"\n",
        "    # Add your code here. This is a one-liner\n",
        "    #print(\"A(x) \", A(x))\n",
        "    new_x = b + x - A(x)\n",
        "    #print(\"new_x \", new_x)\n",
        "    return new_x\n",
        "  return successive_approximation(np.zeros(b.shape), operator, default_termination)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ38sPm4TTrv",
        "colab_type": "text"
      },
      "source": [
        "Now refer to the course notes or [Rust (1996)](https://doi.org/10.1016/s1574-0021(96)01016-7) for the general structure of Newton-Kantorovich iterations and implement the corresponding operator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkZfRM_zSO8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_newton_kantorovich(operator, gateaux_derivative):\n",
        "  \"\"\" Newton-Kantorovich method with matrix-free iterative solver\n",
        "  \"\"\"\n",
        "  def newton_kantorovich(v):\n",
        "    # One-liner using the above basic_iterative_solver\n",
        "    #print(\"v \", v)\n",
        "    w =  v - operator(v)\n",
        "    #print(\"w \", w)\n",
        "    A = gateaux_derivative(v)\n",
        "    #print(\"A \", A)\n",
        "    gd = basic_iterative_solver(A, w)\n",
        "    new_v = np.subtract(v, gd)\n",
        "    print(\"new_v \", new_v)\n",
        "    return new_v\n",
        "  return newton_kantorovich"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "823YSItRTjEy",
        "colab_type": "text"
      },
      "source": [
        "As usual, we verify that we find the optimal value function when all rewards are one. The following should return true:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VWDl1nhT_pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8ba5b61-5319-441d-ad4b-8974b6dd75d7"
      },
      "source": [
        "smooth_operator = make_smooth_bellman_optimality_operator(P, reward_all_ones, discount, default_temperature)\n",
        "gateaux_derivative = make_gateaux_derivative(P, reward_all_ones, discount, default_temperature)\n",
        "\n",
        "newton_kantorovich_operator = make_newton_kantorovich(smooth_operator, gateaux_derivative)\n",
        "solution = successive_approximation(np.zeros((nstates,)), newton_kantorovich_operator, default_termination)\n",
        "print(\"[1/(1-discount)]*nstates) \", [1/(1-discount)]*nstates)\n",
        "print(np.allclose(solution, [1/(1-discount)]*nstates))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new_v  [1.11111881 1.11111881]\n",
            "new_v  [2.09877998 2.09877998]\n",
            "new_v  [2.97670102 2.97670102]\n",
            "new_v  [3.75707527 3.75707527]\n",
            "new_v  [4.45074128 4.45074128]\n",
            "new_v  [5.06733328 5.06733328]\n",
            "new_v  [5.61541506 5.61541506]\n",
            "new_v  [6.10259887 6.10259887]\n",
            "new_v  [6.53565114 6.53565114]\n",
            "new_v  [6.92058649 6.92058649]\n",
            "new_v  [7.26275125 7.26275125]\n",
            "new_v  [7.5668977 7.5668977]\n",
            "new_v  [7.8372501 7.8372501]\n",
            "new_v  [8.07756335 8.07756335]\n",
            "new_v  [8.29117512 8.29117512]\n",
            "new_v  [8.48105225 8.48105225]\n",
            "new_v  [8.64983193 8.64983193]\n",
            "new_v  [8.7998583 8.7998583]\n",
            "new_v  [8.93321508 8.93321508]\n",
            "new_v  [9.05175444 9.05175444]\n",
            "new_v  [9.15712276 9.15712276]\n",
            "new_v  [9.25078349 9.25078349]\n",
            "new_v  [9.33403747 9.33403747]\n",
            "new_v  [9.40804101 9.40804101]\n",
            "new_v  [9.47382193 9.47382193]\n",
            "new_v  [9.53229386 9.53229386]\n",
            "new_v  [9.58426891 9.58426891]\n",
            "new_v  [9.63046895 9.63046895]\n",
            "new_v  [9.67153566 9.67153566]\n",
            "new_v  [9.7080394 9.7080394]\n",
            "new_v  [9.74048717 9.74048717]\n",
            "new_v  [9.76932963 9.76932963]\n",
            "new_v  [9.79496737 9.79496737]\n",
            "new_v  [9.81775648 9.81775648]\n",
            "new_v  [9.83801346 9.83801346]\n",
            "new_v  [9.85601966 9.85601966]\n",
            "new_v  [9.87202518 9.87202518]\n",
            "new_v  [9.88625231 9.88625231]\n",
            "new_v  [9.89889864 9.89889864]\n",
            "new_v  [9.91013983 9.91013983]\n",
            "new_v  [9.92013199 9.92013199]\n",
            "new_v  [9.92901392 9.92901392]\n",
            "new_v  [9.93690896 9.93690896]\n",
            "new_v  [9.94392678 9.94392678]\n",
            "new_v  [9.95016484 9.95016484]\n",
            "new_v  [9.95570978 9.95570978]\n",
            "new_v  [9.96063861 9.96063861]\n",
            "new_v  [9.9650198 9.9650198]\n",
            "new_v  [9.96891419 9.96891419]\n",
            "new_v  [9.97237587 9.97237587]\n",
            "new_v  [9.97545292 9.97545292]\n",
            "new_v  [9.97818808 9.97818808]\n",
            "new_v  [9.98061932 9.98061932]\n",
            "new_v  [9.98278043 9.98278043]\n",
            "new_v  [9.98470142 9.98470142]\n",
            "new_v  [9.98640896 9.98640896]\n",
            "new_v  [9.98792678 9.98792678]\n",
            "new_v  [9.98927595 9.98927595]\n",
            "new_v  [9.99047521 9.99047521]\n",
            "new_v  [9.99154122 9.99154122]\n",
            "new_v  [9.99248879 9.99248879]\n",
            "new_v  [9.99333107 9.99333107]\n",
            "new_v  [9.99407976 9.99407976]\n",
            "new_v  [9.99474527 9.99474527]\n",
            "new_v  [9.99533683 9.99533683]\n",
            "new_v  [9.99586266 9.99586266]\n",
            "new_v  [9.99633007 9.99633007]\n",
            "new_v  [9.99674554 9.99674554]\n",
            "new_v  [9.99711485 9.99711485]\n",
            "new_v  [9.99744312 9.99744312]\n",
            "new_v  [9.99773492 9.99773492]\n",
            "new_v  [9.9979943 9.9979943]\n",
            "new_v  [9.99822485 9.99822485]\n",
            "new_v  [9.99842979 9.99842979]\n",
            "new_v  [9.99861196 9.99861196]\n",
            "new_v  [9.99877389 9.99877389]\n",
            "new_v  [9.99891783 9.99891783]\n",
            "new_v  [9.99904577 9.99904577]\n",
            "new_v  [9.9991595 9.9991595]\n",
            "new_v  [9.99926059 9.99926059]\n",
            "new_v  [9.99935045 9.99935045]\n",
            "new_v  [9.99943032 9.99943032]\n",
            "new_v  [9.99950132 9.99950132]\n",
            "new_v  [9.99956443 9.99956443]\n",
            "new_v  [9.99962053 9.99962053]\n",
            "new_v  [9.99967039 9.99967039]\n",
            "new_v  [9.99971472 9.99971472]\n",
            "new_v  [9.99975412 9.99975412]\n",
            "new_v  [9.99978914 9.99978914]\n",
            "new_v  [9.99982027 9.99982027]\n",
            "new_v  [9.99984794 9.99984794]\n",
            "new_v  [9.99987254 9.99987254]\n",
            "new_v  [9.9998944 9.9998944]\n",
            "new_v  [9.99991384 9.99991384]\n",
            "new_v  [9.99993111 9.99993111]\n",
            "new_v  [9.99994647 9.99994647]\n",
            "new_v  [9.99996012 9.99996012]\n",
            "new_v  [9.99997225 9.99997225]\n",
            "new_v  [9.99998303 9.99998303]\n",
            "new_v  [9.99999262 9.99999262]\n",
            "new_v  [10.00000114 10.00000114]\n",
            "new_v  [10.00000872 10.00000872]\n",
            "new_v  [10.00001545 10.00001545]\n",
            "new_v  [10.00002143 10.00002143]\n",
            "new_v  [10.00002675 10.00002675]\n",
            "new_v  [10.00003148 10.00003148]\n",
            "new_v  [10.00003568 10.00003568]\n",
            "new_v  [10.00003942 10.00003942]\n",
            "new_v  [10.00004274 10.00004274]\n",
            "new_v  [10.00004569 10.00004569]\n",
            "new_v  [10.00004832 10.00004832]\n",
            "new_v  [10.00005065 10.00005065]\n",
            "new_v  [10.00005272 10.00005272]\n",
            "new_v  [10.00005457 10.00005457]\n",
            "new_v  [10.00005621 10.00005621]\n",
            "new_v  [10.00005766 10.00005766]\n",
            "new_v  [10.00005896 10.00005896]\n",
            "new_v  [10.00006011 10.00006011]\n",
            "new_v  [10.00006113 10.00006113]\n",
            "new_v  [10.00006204 10.00006204]\n",
            "new_v  [10.00006285 10.00006285]\n",
            "new_v  [10.00006357 10.00006357]\n",
            "new_v  [10.0000642 10.0000642]\n",
            "new_v  [10.00006477 10.00006477]\n",
            "new_v  [10.00006528 10.00006528]\n",
            "new_v  [10.00006572 10.00006572]\n",
            "new_v  [10.00006612 10.00006612]\n",
            "new_v  [10.00006648 10.00006648]\n",
            "new_v  [10.00006679 10.00006679]\n",
            "new_v  [10.00006707 10.00006707]\n",
            "new_v  [10.00006732 10.00006732]\n",
            "new_v  [10.00006754 10.00006754]\n",
            "new_v  [10.00006774 10.00006774]\n",
            "new_v  [10.00006791 10.00006791]\n",
            "new_v  [10.00006807 10.00006807]\n",
            "new_v  [10.00006821 10.00006821]\n",
            "new_v  [10.00006833 10.00006833]\n",
            "new_v  [10.00006844 10.00006844]\n",
            "new_v  [10.00006854 10.00006854]\n",
            "new_v  [10.00006862 10.00006862]\n",
            "new_v  [10.0000687 10.0000687]\n",
            "new_v  [10.00006877 10.00006877]\n",
            "new_v  [10.00006883 10.00006883]\n",
            "new_v  [10.00006888 10.00006888]\n",
            "new_v  [10.00006893 10.00006893]\n",
            "new_v  [10.00006897 10.00006897]\n",
            "new_v  [10.00006901 10.00006901]\n",
            "new_v  [10.00006904 10.00006904]\n",
            "new_v  [10.00006907 10.00006907]\n",
            "new_v  [10.0000691 10.0000691]\n",
            "new_v  [10.00006912 10.00006912]\n",
            "new_v  [10.00006914 10.00006914]\n",
            "new_v  [10.00006916 10.00006916]\n",
            "new_v  [10.00006918 10.00006918]\n",
            "new_v  [10.00006919 10.00006919]\n",
            "new_v  [10.00006921 10.00006921]\n",
            "new_v  [10.00006922 10.00006922]\n",
            "new_v  [10.00006923 10.00006923]\n",
            "new_v  [10.00006924 10.00006924]\n",
            "new_v  [10.00006925 10.00006925]\n",
            "new_v  [10.00006925 10.00006925]\n",
            "[1/(1-discount)]*nstates)  [10.000000000000002, 10.000000000000002]\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2MSqa0a73w0",
        "colab_type": "text"
      },
      "source": [
        "# Trajectories in the Polytope\n",
        "\n",
        "Sample $1000$ policies at random. Using either the closed-form expression or via successive approximation, compute the value function corresponding to each of them. Plot the mapping $\\pi \\mapsto v_\\pi$ using [matplotlib.pyplot.scatter](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html) (the x axis is $v(s_0)$ and the y axis is $v(s_1))$. Enumerate all possible deterministic policies, compute their value functions and plot them. \n",
        "\n",
        "To do so, you may want to define a new function which generalizes the above ``direct_policy_evaluation`` to the case where you have a batch of policies (let's say as a multidimensional array of size $n \\times |\\mathcal{S}| \\times |\\mathcal{A}|$ where $n$ is the number of policies). This can be done easily by just changing the indices in the Einstein summation since [numpy.linalg.solve](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html) supports *batched*  linear systems out-of-the-box.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS4QqHzOUFlr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "313324b2-bb0d-4346-b41f-8fedd5960bdc"
      },
      "source": [
        "def batch_policy_evaluation(P, R, discount, policies):\n",
        "  p_pi = np.einsum('ijk,lji->ljk', P, policies)\n",
        "  discounted_p_pi = discount * p_pi\n",
        "  a = np.subtract(np.identity(P.shape[-1]), discounted_p_pi)\n",
        "\n",
        "  r_pi = np.einsum('lij, ij -> li', policies, R)\n",
        "  return np.linalg.solve(a, r_pi)\n",
        "\n",
        "\n",
        "import numpy.random as npr\n",
        "\n",
        "policies = npr.rand(1000, nstates, nactions) \n",
        "policies = policies / np.sum(policies, axis=-1, keepdims=True)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "vs = batch_policy_evaluation(P, reward_all_ones, discount, policies)\n",
        "\n",
        "x = vs[:,0]\n",
        "y = vs[:,1]\n",
        "xlim_min = min(x)\n",
        "xlim_max = max(x)\n",
        "\n",
        "print(xlim_min, xlim_max)\n",
        "\n",
        "ylim_min = min(y)\n",
        "ylim_max = max(y)\n",
        "print(ylim_min, ylim_max)\n",
        "\n",
        "\n",
        "eps = 1e-14\n",
        "plt.xlim(xlim_min-eps, xlim_max+eps)\n",
        "plt.ylim(ylim_min-eps, ylim_max+eps)\n",
        "plt.scatter(x, y)\n",
        "\n",
        "def fill(all_lists, part_list, i):\n",
        "  if i == nstates:\n",
        "    all_lists.append(part_list)\n",
        "  else:\n",
        "    for j in range(nactions):\n",
        "      cur_list = list(part_list)\n",
        "      cur_list.append(j)\n",
        "      fill(all_lists, cur_list, i + 1)\n",
        "\n",
        "deterministic_policies= []\n",
        "fill(deterministic_policies, [], 0)\n",
        "\n",
        "det_policies = []\n",
        "for pol in deterministic_policies:\n",
        "  det_policies += [np.eye(P.shape[-1])[pol]]\n",
        "\n",
        "det_policies = np.stack(det_policies)\n",
        "\n",
        "det_vs = batch_policy_evaluation(P, reward_all_ones, discount, det_policies)\n",
        "det_x = det_vs[:,0]\n",
        "det_y = det_vs[:,1]\n",
        "\n",
        "plt.scatter(det_x, det_y, c='red')\n",
        "plt.show()\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.999999999999979 10.000000000000025\n",
            "9.999999999999984 10.000000000000023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc4AAAD7CAYAAADjJnQaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKRoPg0qUJU_",
        "colab_type": "text"
      },
      "source": [
        "Overlay (with a different color) the trajectories taken by value iteration, smooth value iteration and Newton-Kantorovich. To do so, compute the greedy policy for each iterate and solve (policy evaluation) for its corresponding value function. Plot all such points.\n",
        "\n",
        "To get those iterates, you can call the function ``generate_iterates`` directly and consume the generator using the ``list()`` keyword:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JpBdVzD75xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6813e10f-32d8-4457-8328-a97b7a3e592f"
      },
      "source": [
        "# Example:\n",
        "bellman_optimality_operator = make_bellman_optimality_operator(*mdp)\n",
        "trajectory = generate_iterates(np.zeros((nstates,)), bellman_optimality_operator, default_termination)\n",
        "list(trajectory)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_a  [0.5  0.25]\n",
            "max_a  [0.94775 0.655  ]\n",
            "max_a  [1.35034025 1.05028   ]\n",
            "max_a  [1.71260568 1.41129538]\n",
            "max_a  [2.03863332 1.73710926]\n",
            "max_a  [2.33205627 2.03049566]\n",
            "max_a  [2.5961366  2.29456974]\n",
            "max_a  [2.83380884 2.5322409 ]\n",
            "max_a  [3.04771384 2.74614573]\n",
            "max_a  [3.24022835 2.9386602 ]\n",
            "max_a  [3.4134914  3.11192324]\n",
            "max_a  [3.56942814 3.26785999]\n",
            "max_a  [3.70977122 3.40820306]\n",
            "max_a  [3.83607998 3.53451183]\n",
            "max_a  [3.94975787 3.64818972]\n",
            "max_a  [4.05206797 3.75049982]\n",
            "max_a  [4.14414706 3.84257891]\n",
            "max_a  [4.22701824 3.92545009]\n",
            "max_a  [4.3016023  4.00003415]\n",
            "max_a  [4.36872796 4.0671598 ]\n",
            "max_a  [4.42914105 4.1275729 ]\n",
            "max_a  [4.48351283 4.18194468]\n",
            "max_a  [4.53244743 4.23087928]\n",
            "max_a  [4.57648858 4.27492042]\n",
            "max_a  [4.61612561 4.31455745]\n",
            "max_a  [4.65179893 4.35023078]\n",
            "max_a  [4.68390493 4.38233677]\n",
            "max_a  [4.71280032 4.41123217]\n",
            "max_a  [4.73880617 4.43723802]\n",
            "max_a  [4.76221144 4.46064329]\n",
            "max_a  [4.78327619 4.48170803]\n",
            "max_a  [4.80223445 4.5006663 ]\n",
            "max_a  [4.8192969  4.51772874]\n",
            "max_a  [4.83465309 4.53308494]\n",
            "max_a  [4.84847367 4.54690552]\n",
            "max_a  [4.86091219 4.55934403]\n",
            "max_a  [4.87210686 4.5705387 ]\n",
            "max_a  [4.88218206 4.5806139 ]\n",
            "max_a  [4.89124974 4.58968158]\n",
            "max_a  [4.89941065 4.5978425 ]\n",
            "max_a  [4.90675547 4.60518732]\n",
            "max_a  [4.91336581 4.61179766]\n",
            "max_a  [4.91931512 4.61774696]\n",
            "max_a  [4.92466949 4.62310134]\n",
            "max_a  [4.92948843 4.62792028]\n",
            "max_a  [4.93382547 4.63225732]\n",
            "max_a  [4.93772881 4.63616066]\n",
            "max_a  [4.94124182 4.63967366]\n",
            "max_a  [4.94440352 4.64283537]\n",
            "max_a  [4.94724906 4.6456809 ]\n",
            "max_a  [4.94981004 4.64824188]\n",
            "max_a  [4.95211492 4.65054677]\n",
            "max_a  [4.95418932 4.65262116]\n",
            "max_a  [4.95605627 4.65448812]\n",
            "max_a  [4.95773653 4.65616838]\n",
            "max_a  [4.95924876 4.65768061]\n",
            "max_a  [4.96060977 4.65904162]\n",
            "max_a  [4.96183468 4.66026653]\n",
            "max_a  [4.9629371  4.66136895]\n",
            "max_a  [4.96392928 4.66236112]\n",
            "max_a  [4.96482224 4.66325408]\n",
            "max_a  [4.9656259  4.66405775]\n",
            "max_a  [4.9663492  4.66478104]\n",
            "max_a  [4.96700016 4.66543201]\n",
            "max_a  [4.96758603 4.66601788]\n",
            "max_a  [4.96811332 4.66654516]\n",
            "max_a  [4.96858787 4.66701972]\n",
            "max_a  [4.96901497 4.66744682]\n",
            "max_a  [4.96939936 4.66783121]\n",
            "max_a  [4.96974531 4.66817716]\n",
            "max_a  [4.97005667 4.66848851]\n",
            "max_a  [4.97033689 4.66876873]\n",
            "max_a  [4.97058908 4.66902093]\n",
            "max_a  [4.97081606 4.66924791]\n",
            "max_a  [4.97102034 4.66945219]\n",
            "max_a  [4.9712042  4.66963604]\n",
            "max_a  [4.97136966 4.66980151]\n",
            "max_a  [4.97151858 4.66995043]\n",
            "max_a  [4.97165261 4.67008446]\n",
            "max_a  [4.97177324 4.67020508]\n",
            "max_a  [4.9718818  4.67031365]\n",
            "max_a  [4.97197951 4.67041135]\n",
            "max_a  [4.97206744 4.67049929]\n",
            "max_a  [4.97214658 4.67057843]\n",
            "max_a  [4.97221781 4.67064966]\n",
            "max_a  [4.97228192 4.67071376]\n",
            "max_a  [4.97233961 4.67077146]\n",
            "max_a  [4.97239154 4.67082338]\n",
            "max_a  [4.97243827 4.67087012]\n",
            "max_a  [4.97248033 4.67091218]\n",
            "max_a  [4.97251818 4.67095003]\n",
            "max_a  [4.97255225 4.6709841 ]\n",
            "max_a  [4.97258291 4.67101476]\n",
            "max_a  [4.97261051 4.67104235]\n",
            "max_a  [4.97263534 4.67106719]\n",
            "max_a  [4.9726577  4.67108954]\n",
            "max_a  [4.97267781 4.67110966]\n",
            "max_a  [4.97269592 4.67112776]\n",
            "max_a  [4.97271221 4.67114406]\n",
            "max_a  [4.97272688 4.67115872]\n",
            "max_a  [4.97274008 4.67117192]\n",
            "max_a  [4.97275196 4.6711838 ]\n",
            "max_a  [4.97276265 4.67119449]\n",
            "max_a  [4.97277227 4.67120411]\n",
            "max_a  [4.97278093 4.67121277]\n",
            "max_a  [4.97278872 4.67122057]\n",
            "max_a  [4.97279574 4.67122758]\n",
            "max_a  [4.97280205 4.6712339 ]\n",
            "max_a  [4.97280773 4.67123958]\n",
            "max_a  [4.97281285 4.67124469]\n",
            "max_a  [4.97281745 4.67124929]\n",
            "max_a  [4.97282159 4.67125343]\n",
            "max_a  [4.97282532 4.67125716]\n",
            "max_a  [4.97282867 4.67126052]\n",
            "max_a  [4.97283169 4.67126354]\n",
            "max_a  [4.97283441 4.67126625]\n",
            "max_a  [4.97283685 4.6712687 ]\n",
            "max_a  [4.97283906 4.6712709 ]\n",
            "max_a  [4.97284104 4.67127288]\n",
            "max_a  [4.97284282 4.67127467]\n",
            "max_a  [4.97284442 4.67127627]\n",
            "max_a  [4.97284587 4.67127771]\n",
            "max_a  [4.97284717 4.67127901]\n",
            "max_a  [4.97284834 4.67128018]\n",
            "max_a  [4.97284939 4.67128124]\n",
            "max_a  [4.97285034 4.67128218]\n",
            "max_a  [4.97285119 4.67128304]\n",
            "max_a  [4.97285196 4.6712838 ]\n",
            "max_a  [4.97285265 4.67128449]\n",
            "max_a  [4.97285327 4.67128512]\n",
            "max_a  [4.97285383 4.67128568]\n",
            "max_a  [4.97285433 4.67128618]\n",
            "max_a  [4.97285479 4.67128663]\n",
            "max_a  [4.9728552  4.67128704]\n",
            "max_a  [4.97285556 4.67128741]\n",
            "max_a  [4.97285589 4.67128774]\n",
            "max_a  [4.97285619 4.67128804]\n",
            "max_a  [4.97285646 4.6712883 ]\n",
            "max_a  [4.9728567  4.67128854]\n",
            "max_a  [4.97285692 4.67128876]\n",
            "max_a  [4.97285711 4.67128896]\n",
            "max_a  [4.97285729 4.67128913]\n",
            "max_a  [4.97285744 4.67128929]\n",
            "max_a  [4.97285759 4.67128943]\n",
            "max_a  [4.97285771 4.67128956]\n",
            "max_a  [4.97285783 4.67128967]\n",
            "max_a  [4.97285793 4.67128978]\n",
            "max_a  [4.97285803 4.67128987]\n",
            "max_a  [4.97285811 4.67128996]\n",
            "max_a  [4.97285819 4.67129003]\n",
            "max_a  [4.97285825 4.6712901 ]\n",
            "max_a  [4.97285832 4.67129016]\n",
            "max_a  [4.97285837 4.67129022]\n",
            "max_a  [4.97285842 4.67129027]\n",
            "max_a  [4.97285846 4.67129031]\n",
            "max_a  [4.9728585  4.67129035]\n",
            "max_a  [4.97285854 4.67129039]\n",
            "max_a  [4.97285857 4.67129042]\n",
            "max_a  [4.9728586  4.67129045]\n",
            "max_a  [4.97285863 4.67129047]\n",
            "max_a  [4.97285865 4.6712905 ]\n",
            "max_a  [4.97285867 4.67129052]\n",
            "max_a  [4.97285869 4.67129054]\n",
            "max_a  [4.97285871 4.67129056]\n",
            "max_a  [4.97285873 4.67129057]\n",
            "max_a  [4.97285874 4.67129059]\n",
            "max_a  [4.97285875 4.6712906 ]\n",
            "max_a  [4.97285876 4.67129061]\n",
            "max_a  [4.97285877 4.67129062]\n",
            "max_a  [4.97285878 4.67129063]\n",
            "max_a  [4.97285879 4.67129064]\n",
            "max_a  [4.9728588  4.67129064]\n",
            "max_a  [4.97285881 4.67129065]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DeviceArray([0.5 , 0.25], dtype=float64),\n",
              " DeviceArray([0.94775, 0.655  ], dtype=float64),\n",
              " DeviceArray([1.35034025, 1.05028   ], dtype=float64),\n",
              " DeviceArray([1.71260568, 1.41129538], dtype=float64),\n",
              " DeviceArray([2.03863332, 1.73710926], dtype=float64),\n",
              " DeviceArray([2.33205627, 2.03049566], dtype=float64),\n",
              " DeviceArray([2.5961366 , 2.29456974], dtype=float64),\n",
              " DeviceArray([2.83380884, 2.5322409 ], dtype=float64),\n",
              " DeviceArray([3.04771384, 2.74614573], dtype=float64),\n",
              " DeviceArray([3.24022835, 2.9386602 ], dtype=float64),\n",
              " DeviceArray([3.4134914 , 3.11192324], dtype=float64),\n",
              " DeviceArray([3.56942814, 3.26785999], dtype=float64),\n",
              " DeviceArray([3.70977122, 3.40820306], dtype=float64),\n",
              " DeviceArray([3.83607998, 3.53451183], dtype=float64),\n",
              " DeviceArray([3.94975787, 3.64818972], dtype=float64),\n",
              " DeviceArray([4.05206797, 3.75049982], dtype=float64),\n",
              " DeviceArray([4.14414706, 3.84257891], dtype=float64),\n",
              " DeviceArray([4.22701824, 3.92545009], dtype=float64),\n",
              " DeviceArray([4.3016023 , 4.00003415], dtype=float64),\n",
              " DeviceArray([4.36872796, 4.0671598 ], dtype=float64),\n",
              " DeviceArray([4.42914105, 4.1275729 ], dtype=float64),\n",
              " DeviceArray([4.48351283, 4.18194468], dtype=float64),\n",
              " DeviceArray([4.53244743, 4.23087928], dtype=float64),\n",
              " DeviceArray([4.57648858, 4.27492042], dtype=float64),\n",
              " DeviceArray([4.61612561, 4.31455745], dtype=float64),\n",
              " DeviceArray([4.65179893, 4.35023078], dtype=float64),\n",
              " DeviceArray([4.68390493, 4.38233677], dtype=float64),\n",
              " DeviceArray([4.71280032, 4.41123217], dtype=float64),\n",
              " DeviceArray([4.73880617, 4.43723802], dtype=float64),\n",
              " DeviceArray([4.76221144, 4.46064329], dtype=float64),\n",
              " DeviceArray([4.78327619, 4.48170803], dtype=float64),\n",
              " DeviceArray([4.80223445, 4.5006663 ], dtype=float64),\n",
              " DeviceArray([4.8192969 , 4.51772874], dtype=float64),\n",
              " DeviceArray([4.83465309, 4.53308494], dtype=float64),\n",
              " DeviceArray([4.84847367, 4.54690552], dtype=float64),\n",
              " DeviceArray([4.86091219, 4.55934403], dtype=float64),\n",
              " DeviceArray([4.87210686, 4.5705387 ], dtype=float64),\n",
              " DeviceArray([4.88218206, 4.5806139 ], dtype=float64),\n",
              " DeviceArray([4.89124974, 4.58968158], dtype=float64),\n",
              " DeviceArray([4.89941065, 4.5978425 ], dtype=float64),\n",
              " DeviceArray([4.90675547, 4.60518732], dtype=float64),\n",
              " DeviceArray([4.91336581, 4.61179766], dtype=float64),\n",
              " DeviceArray([4.91931512, 4.61774696], dtype=float64),\n",
              " DeviceArray([4.92466949, 4.62310134], dtype=float64),\n",
              " DeviceArray([4.92948843, 4.62792028], dtype=float64),\n",
              " DeviceArray([4.93382547, 4.63225732], dtype=float64),\n",
              " DeviceArray([4.93772881, 4.63616066], dtype=float64),\n",
              " DeviceArray([4.94124182, 4.63967366], dtype=float64),\n",
              " DeviceArray([4.94440352, 4.64283537], dtype=float64),\n",
              " DeviceArray([4.94724906, 4.6456809 ], dtype=float64),\n",
              " DeviceArray([4.94981004, 4.64824188], dtype=float64),\n",
              " DeviceArray([4.95211492, 4.65054677], dtype=float64),\n",
              " DeviceArray([4.95418932, 4.65262116], dtype=float64),\n",
              " DeviceArray([4.95605627, 4.65448812], dtype=float64),\n",
              " DeviceArray([4.95773653, 4.65616838], dtype=float64),\n",
              " DeviceArray([4.95924876, 4.65768061], dtype=float64),\n",
              " DeviceArray([4.96060977, 4.65904162], dtype=float64),\n",
              " DeviceArray([4.96183468, 4.66026653], dtype=float64),\n",
              " DeviceArray([4.9629371 , 4.66136895], dtype=float64),\n",
              " DeviceArray([4.96392928, 4.66236112], dtype=float64),\n",
              " DeviceArray([4.96482224, 4.66325408], dtype=float64),\n",
              " DeviceArray([4.9656259 , 4.66405775], dtype=float64),\n",
              " DeviceArray([4.9663492 , 4.66478104], dtype=float64),\n",
              " DeviceArray([4.96700016, 4.66543201], dtype=float64),\n",
              " DeviceArray([4.96758603, 4.66601788], dtype=float64),\n",
              " DeviceArray([4.96811332, 4.66654516], dtype=float64),\n",
              " DeviceArray([4.96858787, 4.66701972], dtype=float64),\n",
              " DeviceArray([4.96901497, 4.66744682], dtype=float64),\n",
              " DeviceArray([4.96939936, 4.66783121], dtype=float64),\n",
              " DeviceArray([4.96974531, 4.66817716], dtype=float64),\n",
              " DeviceArray([4.97005667, 4.66848851], dtype=float64),\n",
              " DeviceArray([4.97033689, 4.66876873], dtype=float64),\n",
              " DeviceArray([4.97058908, 4.66902093], dtype=float64),\n",
              " DeviceArray([4.97081606, 4.66924791], dtype=float64),\n",
              " DeviceArray([4.97102034, 4.66945219], dtype=float64),\n",
              " DeviceArray([4.9712042 , 4.66963604], dtype=float64),\n",
              " DeviceArray([4.97136966, 4.66980151], dtype=float64),\n",
              " DeviceArray([4.97151858, 4.66995043], dtype=float64),\n",
              " DeviceArray([4.97165261, 4.67008446], dtype=float64),\n",
              " DeviceArray([4.97177324, 4.67020508], dtype=float64),\n",
              " DeviceArray([4.9718818 , 4.67031365], dtype=float64),\n",
              " DeviceArray([4.97197951, 4.67041135], dtype=float64),\n",
              " DeviceArray([4.97206744, 4.67049929], dtype=float64),\n",
              " DeviceArray([4.97214658, 4.67057843], dtype=float64),\n",
              " DeviceArray([4.97221781, 4.67064966], dtype=float64),\n",
              " DeviceArray([4.97228192, 4.67071376], dtype=float64),\n",
              " DeviceArray([4.97233961, 4.67077146], dtype=float64),\n",
              " DeviceArray([4.97239154, 4.67082338], dtype=float64),\n",
              " DeviceArray([4.97243827, 4.67087012], dtype=float64),\n",
              " DeviceArray([4.97248033, 4.67091218], dtype=float64),\n",
              " DeviceArray([4.97251818, 4.67095003], dtype=float64),\n",
              " DeviceArray([4.97255225, 4.6709841 ], dtype=float64),\n",
              " DeviceArray([4.97258291, 4.67101476], dtype=float64),\n",
              " DeviceArray([4.97261051, 4.67104235], dtype=float64),\n",
              " DeviceArray([4.97263534, 4.67106719], dtype=float64),\n",
              " DeviceArray([4.9726577 , 4.67108954], dtype=float64),\n",
              " DeviceArray([4.97267781, 4.67110966], dtype=float64),\n",
              " DeviceArray([4.97269592, 4.67112776], dtype=float64),\n",
              " DeviceArray([4.97271221, 4.67114406], dtype=float64),\n",
              " DeviceArray([4.97272688, 4.67115872], dtype=float64),\n",
              " DeviceArray([4.97274008, 4.67117192], dtype=float64),\n",
              " DeviceArray([4.97275196, 4.6711838 ], dtype=float64),\n",
              " DeviceArray([4.97276265, 4.67119449], dtype=float64),\n",
              " DeviceArray([4.97277227, 4.67120411], dtype=float64),\n",
              " DeviceArray([4.97278093, 4.67121277], dtype=float64),\n",
              " DeviceArray([4.97278872, 4.67122057], dtype=float64),\n",
              " DeviceArray([4.97279574, 4.67122758], dtype=float64),\n",
              " DeviceArray([4.97280205, 4.6712339 ], dtype=float64),\n",
              " DeviceArray([4.97280773, 4.67123958], dtype=float64),\n",
              " DeviceArray([4.97281285, 4.67124469], dtype=float64),\n",
              " DeviceArray([4.97281745, 4.67124929], dtype=float64),\n",
              " DeviceArray([4.97282159, 4.67125343], dtype=float64),\n",
              " DeviceArray([4.97282532, 4.67125716], dtype=float64),\n",
              " DeviceArray([4.97282867, 4.67126052], dtype=float64),\n",
              " DeviceArray([4.97283169, 4.67126354], dtype=float64),\n",
              " DeviceArray([4.97283441, 4.67126625], dtype=float64),\n",
              " DeviceArray([4.97283685, 4.6712687 ], dtype=float64),\n",
              " DeviceArray([4.97283906, 4.6712709 ], dtype=float64),\n",
              " DeviceArray([4.97284104, 4.67127288], dtype=float64),\n",
              " DeviceArray([4.97284282, 4.67127467], dtype=float64),\n",
              " DeviceArray([4.97284442, 4.67127627], dtype=float64),\n",
              " DeviceArray([4.97284587, 4.67127771], dtype=float64),\n",
              " DeviceArray([4.97284717, 4.67127901], dtype=float64),\n",
              " DeviceArray([4.97284834, 4.67128018], dtype=float64),\n",
              " DeviceArray([4.97284939, 4.67128124], dtype=float64),\n",
              " DeviceArray([4.97285034, 4.67128218], dtype=float64),\n",
              " DeviceArray([4.97285119, 4.67128304], dtype=float64),\n",
              " DeviceArray([4.97285196, 4.6712838 ], dtype=float64),\n",
              " DeviceArray([4.97285265, 4.67128449], dtype=float64),\n",
              " DeviceArray([4.97285327, 4.67128512], dtype=float64),\n",
              " DeviceArray([4.97285383, 4.67128568], dtype=float64),\n",
              " DeviceArray([4.97285433, 4.67128618], dtype=float64),\n",
              " DeviceArray([4.97285479, 4.67128663], dtype=float64),\n",
              " DeviceArray([4.9728552 , 4.67128704], dtype=float64),\n",
              " DeviceArray([4.97285556, 4.67128741], dtype=float64),\n",
              " DeviceArray([4.97285589, 4.67128774], dtype=float64),\n",
              " DeviceArray([4.97285619, 4.67128804], dtype=float64),\n",
              " DeviceArray([4.97285646, 4.6712883 ], dtype=float64),\n",
              " DeviceArray([4.9728567 , 4.67128854], dtype=float64),\n",
              " DeviceArray([4.97285692, 4.67128876], dtype=float64),\n",
              " DeviceArray([4.97285711, 4.67128896], dtype=float64),\n",
              " DeviceArray([4.97285729, 4.67128913], dtype=float64),\n",
              " DeviceArray([4.97285744, 4.67128929], dtype=float64),\n",
              " DeviceArray([4.97285759, 4.67128943], dtype=float64),\n",
              " DeviceArray([4.97285771, 4.67128956], dtype=float64),\n",
              " DeviceArray([4.97285783, 4.67128967], dtype=float64),\n",
              " DeviceArray([4.97285793, 4.67128978], dtype=float64),\n",
              " DeviceArray([4.97285803, 4.67128987], dtype=float64),\n",
              " DeviceArray([4.97285811, 4.67128996], dtype=float64),\n",
              " DeviceArray([4.97285819, 4.67129003], dtype=float64),\n",
              " DeviceArray([4.97285825, 4.6712901 ], dtype=float64),\n",
              " DeviceArray([4.97285832, 4.67129016], dtype=float64),\n",
              " DeviceArray([4.97285837, 4.67129022], dtype=float64),\n",
              " DeviceArray([4.97285842, 4.67129027], dtype=float64),\n",
              " DeviceArray([4.97285846, 4.67129031], dtype=float64),\n",
              " DeviceArray([4.9728585 , 4.67129035], dtype=float64),\n",
              " DeviceArray([4.97285854, 4.67129039], dtype=float64),\n",
              " DeviceArray([4.97285857, 4.67129042], dtype=float64),\n",
              " DeviceArray([4.9728586 , 4.67129045], dtype=float64),\n",
              " DeviceArray([4.97285863, 4.67129047], dtype=float64),\n",
              " DeviceArray([4.97285865, 4.6712905 ], dtype=float64),\n",
              " DeviceArray([4.97285867, 4.67129052], dtype=float64),\n",
              " DeviceArray([4.97285869, 4.67129054], dtype=float64),\n",
              " DeviceArray([4.97285871, 4.67129056], dtype=float64),\n",
              " DeviceArray([4.97285873, 4.67129057], dtype=float64),\n",
              " DeviceArray([4.97285874, 4.67129059], dtype=float64),\n",
              " DeviceArray([4.97285875, 4.6712906 ], dtype=float64),\n",
              " DeviceArray([4.97285876, 4.67129061], dtype=float64),\n",
              " DeviceArray([4.97285877, 4.67129062], dtype=float64),\n",
              " DeviceArray([4.97285878, 4.67129063], dtype=float64),\n",
              " DeviceArray([4.97285879, 4.67129064], dtype=float64),\n",
              " DeviceArray([4.9728588 , 4.67129064], dtype=float64),\n",
              " DeviceArray([4.97285881, 4.67129065], dtype=float64)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04jm7-x1APoo",
        "colab_type": "text"
      },
      "source": [
        "Describes what happens as you decrease the temperature in smooth value iteration and with Newton-Kantorovich and the smooth Bellman operator? Answer this question by creating a new polytope plot on which you overlays the different trajectories taken by your algorithm depending on the temperature parameter. Don't forget to add a legend. \n",
        "\n",
        "Also, compare the trajectories taken by policy iteration and the Newton-Kantorovich method. Do they match? Compare the iterates computed by smooth policy iteration and those of the Newton-Kantorovich method with the smooth operator using the ``generate_iterates`` function. For each iterate of smooth policy iteration (a policy), compute the associated value function and compare with the values produced by Newton-Kantorovich. Do you get the same sequence of values as if you were to run Newton-Kantorovich? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FccNFU_t6Ami",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}